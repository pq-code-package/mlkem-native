///
/// Copyright (c) 2025 Arm Limited
/// SPDX-License-Identifier: Apache-2.0 OR MIT OR ISC
///
#include "../../../../common.h"
#if defined(MLK_FIPS202_ARMV81M_NEED_X4) && \
    !defined(MLK_CONFIG_MULTILEVEL_NO_SHARED)

 .thumb
 .syntax unified
.text

.macro deinterleave_inner t
    vsli.u32     \t, \t, #8
    vsli.u16     \t, \t, #4
    vshr.u8       u, \t, #3
    vsli.u8      \t,  u, #4
    vshr.u8       u, \t, #2
    vsli.u8      \t,  u, #3
    vshr.u8       u, \t, #1
    vsli.u8      \t,  u, #2
.endm

.align 8
.type MLK_ASM_NAMESPACE(from_bit_interleaving_4x), %function
.global MLK_ASM_NAMESPACE(from_bit_interleaving_4x)
MLK_ASM_FN_SYMBOL(from_bit_interleaving_4x)
    push	{r4 - r11, lr}
    vpush   {d8 - d15}
        from_bit_interleaving_4x_start:
                                    // Instructions:    44
                                    // Expected cycles: 85
                                    // Expected IPC:    0.52
                                    //
                                    // Cycle bound:     85.0
                                    // IPC bound:       0.52
                                    //
                                    // Wall time:     0.04s
                                    // User time:     0.04s
                                    //
                                    // --------------------------------- cycle (expected) --------------------------------->
                                    // 0                        25                       50                       75
                                    // |------------------------|------------------------|------------------------|---------
        vshr.u32 q2, q0, #16        // *....................................................................................
        mov r6, #0x55               // .*...................................................................................
        vsli.u32 q0, q0, #8         // ..*..................................................................................
        vshr.u32 q6, q1, #16        // ....*................................................................................
        vsli.u32 q1, q1, #8         // ......*..............................................................................
        vsli.u32 q2, q2, #8         // ........*............................................................................
        vsli.u16 q0, q0, #4         // ..........*..........................................................................
        vsli.u32 q6, q6, #8         // ............*........................................................................
        vsli.u16 q1, q1, #4         // ..............*......................................................................
        vsli.u16 q2, q2, #4         // ................*....................................................................
        vshr.u8 q5, q0, #3          // ..................*..................................................................
        vsli.u8 q0, q5, #4          // ....................*................................................................
        vsli.u16 q6, q6, #4         // ......................*..............................................................
        vshr.u8 q7, q1, #3          // ........................*............................................................
        vsli.u8 q1, q7, #4          // ..........................*..........................................................
        vshr.u8 q5, q2, #3          // ............................*........................................................
        vsli.u8 q2, q5, #4          // ..............................*......................................................
        vshr.u8 q4, q0, #2          // ................................*....................................................
        vsli.u8 q0, q4, #3          // ..................................*..................................................
        vshr.u8 q3, q6, #3          // ....................................*................................................
        vsli.u8 q6, q3, #4          // ......................................*..............................................
        vshr.u8 q7, q1, #2          // ........................................*............................................
        vsli.u8 q1, q7, #3          // ..........................................*..........................................
        vshr.u8 q4, q2, #2          // ............................................*........................................
        vsli.u8 q2, q4, #3          // ..............................................*......................................
        vshr.u8 q5, q0, #1          // ................................................*....................................
        vsli.u8 q0, q5, #2          // ..................................................*..................................
        vshr.u8 q7, q6, #2          // ....................................................*................................
        vsli.u8 q6, q7, #3          // ......................................................*..............................
        vshr.u8 q5, q1, #1          // ........................................................*............................
        vsli.u8 q1, q5, #2          // ..........................................................*..........................
        vshr.u8 q4, q2, #1          // ............................................................*........................
        vsli.u8 q2, q4, #2          // ..............................................................*......................
        vdup.u8 q4, r6              // ................................................................*....................
        vand.u32 q7, q0, q4         // ..................................................................*..................
        vshr.u8 q3, q6, #1          // ....................................................................*................
        vsli.u8 q6, q3, #2          // ......................................................................*..............
        vand.u32 q1, q1, q4         // ........................................................................*............
        vand.u32 q3, q2, q4         // ..........................................................................*..........
        vand.u32 q6, q6, q4         // ............................................................................*........
        vshl.u32 q2, q1, #1         // ..............................................................................*......
        vorr q0, q7, q2             // ................................................................................*....
        vshl.u32 q2, q6, #1         // ..................................................................................*..
        vorr q1, q3, q2             // ....................................................................................*

                                         // --------------------------------- cycle (expected) --------------------------------->
                                         // 0                        25                       50                       75
                                         // |------------------------|------------------------|------------------------|---------
        // vshr.u32     E1, q0, #16      // *....................................................................................
        // vsli.u32     E1, E1, #8       // ........*............................................................................
        // vsli.u16     E1, E1, #4       // ................*....................................................................
        // vshr.u8       u, E1, #3       // ............................*........................................................
        // vsli.u8      E1,  u, #4       // ..............................*......................................................
        // vshr.u8       u, E1, #2       // ............................................*........................................
        // vsli.u8      E1,  u, #3       // ..............................................*......................................
        // vshr.u8       u, E1, #1       // ............................................................*........................
        // vsli.u8      E1,  u, #2       // ..............................................................*......................
        // vshr.u32     O1, q1, #16      // ....*................................................................................
        // vsli.u32     O1, O1, #8       // ............*........................................................................
        // vsli.u16     O1, O1, #4       // ......................*..............................................................
        // vshr.u8       u, O1, #3       // ....................................*................................................
        // vsli.u8      O1,  u, #4       // ......................................*..............................................
        // vshr.u8       u, O1, #2       // ....................................................*................................
        // vsli.u8      O1,  u, #3       // ......................................................*..............................
        // vshr.u8       u, O1, #1       // ....................................................................*................
        // vsli.u8      O1,  u, #2       // ......................................................................*..............
        // mov          r, #0x55         // .*...................................................................................
        // vdup.u8      M, r             // ................................................................*....................
        // vand.u32     E1, E1, M        // ..........................................................................*..........
        // vand.u32     O1, O1, M        // ............................................................................*........
        // vsli.u32     q0, q0, #8       // ..*..................................................................................
        // vsli.u16     q0, q0, #4       // ..........*..........................................................................
        // vshr.u8       u, q0, #3       // ..................*..................................................................
        // vsli.u8      q0,  u, #4       // ....................*................................................................
        // vshr.u8       u, q0, #2       // ................................*....................................................
        // vsli.u8      q0,  u, #3       // ..................................*..................................................
        // vshr.u8       u, q0, #1       // ................................................*....................................
        // vsli.u8      q0,  u, #2       // ..................................................*..................................
        // vsli.u32     q1, q1, #8       // ......*..............................................................................
        // vsli.u16     q1, q1, #4       // ..............*......................................................................
        // vshr.u8       u, q1, #3       // ........................*............................................................
        // vsli.u8      q1,  u, #4       // ..........................*..........................................................
        // vshr.u8       u, q1, #2       // ........................................*............................................
        // vsli.u8      q1,  u, #3       // ..........................................*..........................................
        // vshr.u8       u, q1, #1       // ........................................................*............................
        // vsli.u8      q1,  u, #2       // ..........................................................*..........................
        // vand.u32     q0, q0, M        // ..................................................................*..................
        // vand.u32     q1, q1, M        // ........................................................................*............
        // vshl.u32     q1, q1, #1       // ..............................................................................*......
        // vorr         q0, q0, q1       // ................................................................................*....
        // vshl.u32     O1, O1, #1       // ..................................................................................*..
        // vorr         q1, E1, O1       // ....................................................................................*

        from_bit_interleaving_4x_exit:

    vpop {d8 - d15}
    pop {r4 - r11, pc}


#endif /* MLK_FIPS202_ARMV81M_NEED_X4 && !MLK_CONFIG_MULTILEVEL_NO_SHARED */
