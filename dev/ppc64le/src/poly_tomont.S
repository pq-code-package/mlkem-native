/*
 * Copyright (c) The mlkem-native project authors
 * SPDX-License-Identifier: Apache-2.0 OR ISC OR MIT
 */

/*
 * Copyright 2025- IBM Corp.
 *
 * ===================================================================================
 * Written by Danny Tsen <dtsen@us.ibm.com>
 */

/*
 * Poly_tomont: Inplace conversion of all coefficients of a polynomial
 *              from normal domain to Montgomery domain
 *
 * Arguments:*r: pointer to input/output polynomial
 */

#include "../../../common.h"
#if defined(MLK_ARITH_BACKEND_PPC64LE_DEFAULT) && \
    !defined(MLK_CONFIG_MULTILEVEL_NO_SHARED)
/* simpasm: header-end */

#include "consts.h"

#define V1353   0
#define V_QINV  2
#define V_NMKQ  5

.machine "any"
.text

/*
 * montgomery_reduce
 * t = a * QINV
 * t = (a - (int32_t)t*_MLKEM_Q) >> 16
 *
 * -----------------------------------
 * MREDUCE_4X(_v0, _v1, _v2, _v3)
 */
.macro MREDUCE_4X _v0 _v1 _v2 _v3
        lxvd2x  32+v13, 0, r3
        addi    r3, r3, 16
        lxvd2x  32+v18, 0, r3
        addi    r3, r3, 16
        lxvd2x  32+v23, 0, r3
        addi    r3, r3, 16
        lxvd2x  32+v7, 0, r3
        addi    r3, r3, 16

        vmladduhm v15, v13, V1353, v3
        vmladduhm v20, v18, V1353, v3
        vmladduhm v25, v23, V1353, v3
        vmladduhm v9, v7, V1353, v3

        vmhraddshs v14, v13, V1353, v3
        vmhraddshs v19, v18, V1353, v3
        vmhraddshs v24, v23, V1353, v3
        vmhraddshs v8, v7, V1353, v3

        vmladduhm v15, v15, V_QINV, v3
        vmladduhm v20, v20, V_QINV, v3
        vmladduhm v25, v25, V_QINV, v3
        vmladduhm v9, v9, V_QINV, v3

        vmhraddshs v15, v15, V_NMKQ, v14
        vmhraddshs v20, v20, V_NMKQ, v19
        vmhraddshs v25, v25, V_NMKQ, v24
        vmhraddshs v9, v9, V_NMKQ, v8

        /* Shift right 1 bit */
        vsrah \_v0, v15, v4
        vsrah \_v1, v20, v4
        vsrah \_v2, v25, v4
        vsrah \_v3, v9, v4
.endm

.macro Write_8X
        stxvd2x 32+v27, r4, r3
        stxvd2x 32+v28, r5, r3
        stxvd2x 32+v29, r6, r3
        stxvd2x 32+v30, r7, r3
        stxvd2x 32+v13, r8, r3
        stxvd2x 32+v18, r9, r3
        stxvd2x 32+v23, r10, r3
        stxvd2x 32+v7, r11, r3
.endm

.align 4
.globl MLK_ASM_NAMESPACE(poly_tomont_ppc)
MLK_ASM_FN_SYMBOL(poly_tomont_ppc)
        stdu    r1, -320(r1)
        mflr    r0

        li      r6, 128
        li      r7, 144
        li      r8, 160
        li      r9, 176
        li      r10, 192
        li      r11, 208
        li      r12, 224
        stxvx   32+v20, r6, r1
        stxvx   32+v21, r7, r1
        stxvx   32+v22, r8, r1
        stxvx   32+v23, r9, r1
        stxvx   32+v24, r10, r1
        stxvx   32+v25, r11, r1
        stxvx   32+v26, r12, r1
        li      r6, 240
        li      r7, 256
        li      r8, 272
        li      r9, 288
        stxvx   32+v27, r6, r1
        stxvx   32+v28, r7, r1
        stxvx   32+v29, r8, r1
        stxvx   32+v30, r9, r1

        li      r6, NQ_OFFSET
        li      r7, QINV_OFFSET
        li      r8, C1353_OFFSET
        lxvx    32+V_NMKQ, r6, r4
        lxvx    32+V_QINV, r7, r4
        lxvx    32+V1353, r8, r4

        vxor    v3, v3, v3
        vspltish v4, 1

        li      r4, -128
        li      r5, -112
        li      r6, -96
        li      r7, -80
        li      r8, -64
        li      r9, -48
        li      r10, -32
        li      r11, -16

        MREDUCE_4X v27, v28, v29, v30
        MREDUCE_4X v13, v18, v23, v7
        Write_8X

        MREDUCE_4X v27, v28, v29, v30
        MREDUCE_4X v13, v18, v23, v7
        Write_8X

        MREDUCE_4X v27, v28, v29, v30
        MREDUCE_4X v13, v18, v23, v7
        Write_8X

        MREDUCE_4X v27, v28, v29, v30
        MREDUCE_4X v13, v18, v23, v7
        Write_8X

        li      r6, 128
        li      r7, 144
        li      r8, 160
        li      r9, 176
        li      r10, 192
        li      r11, 208
        li      r12, 224
        lxvx    32+v20, r6, r1
        lxvx    32+v21, r7, r1
        lxvx    32+v22, r8, r1
        lxvx    32+v23, r9, r1
        lxvx    32+v24, r10, r1
        lxvx    32+v25, r11, r1
        lxvx    32+v26, r12, r1
        li      r6, 240
        li      r7, 256
        li      r8, 272
        li      r9, 288
        lxvx    32+v27, r6, r1
        lxvx    32+v28, r7, r1
        lxvx    32+v29, r8, r1
        lxvx    32+v30, r9, r1
        mtlr    r0
        addi    r1, r1, 320
        blr

/* To facilitate single-compilation-unit (SCU) builds, undefine all macros.
 * Don't modify by hand -- this is auto-generated by scripts/autogen. */
#undef V1353
#undef V_QINV
#undef V_NMKQ

/* simpasm: footer-start */
#endif /* MLK_ARITH_BACKEND_PPC64LE_DEFAULT && \
          !MLK_CONFIG_MULTILEVEL_NO_SHARED */

/* To facilitate single-compilation-unit (SCU) builds, undefine all macros.
 * Don't modify by hand -- this is auto-generated by scripts/autogen. */
#undef V1353
#undef V_QINV
#undef V_NMKQ
