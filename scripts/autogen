#!/usr/bin/env python3
# Copyright (c) The mlkem-native project authors
# SPDX-License-Identifier: Apache-2.0 OR ISC OR MIT

import subprocess
import tempfile
import platform
import argparse
import shutil
import pathlib
import re
import sys
import pyparsing as pp
import os
import yaml

from concurrent.futures import ThreadPoolExecutor
from functools import partial

modulus = 3329
root_of_unity = 17
montgomery_factor = pow(2, 16, modulus)

# This file re-generated auto-generated source files in mlkem-native.
#
# It currently covers:
# - zeta values for the reference NTT and invNTT
# - lookup tables used for fast rejection sampling
# - source files for monolithic single-CU build
# - simplified assembly sources
# - header guards
# - #undef's for CU-local macros

# Standard color definitions
GREEN = "\033[32m"
RED = "\033[31m"
BLUE = "\033[94m"
BOLD = "\033[1m"
NORMAL = "\033[0m"


def clear_status_line():
    """Clear any existing status line by overwriting with spaces and returning to start of line"""
    print(f"\r{' ' * 160}", end="", flush=True)


def status_update(task, msg):
    clear_status_line()
    print(f"\r{BLUE}[{task}]{NORMAL}: {msg} ...", end="", flush=True)


def high_level_status(msg):
    clear_status_line()
    print(
        f"\r{GREEN}âœ“{NORMAL} {msg}"
    )  # This will end with a newline, clearing the status line


def info(msg):
    clear_status_line()
    print(f"\r{GREEN}info{NORMAL} {msg}")


def error(msg):
    clear_status_line()
    print(f"\r{RED}error{NORMAL} {msg}")


def file_updated(filename):
    clear_status_line()
    print(f"\r{BOLD}updated {filename}{NORMAL}")


def gen_header():
    yield "/*"
    yield " * Copyright (c) The mlkem-native project authors"
    yield " * SPDX-License-Identifier: Apache-2.0 OR ISC OR MIT"
    yield " */"
    yield ""
    yield "/*"
    yield " * WARNING: This file is auto-generated from scripts/autogen"
    yield " *          in the mlkem-native repository."
    yield " *          Do not modify it directly."
    yield " */"
    yield ""


def gen_hol_light_header():
    yield "(*"
    yield " * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved."
    yield " * SPDX-License-Identifier: Apache-2.0 OR ISC OR MIT-0"
    yield " *)"
    yield ""
    yield "(*"
    yield " * WARNING: This file is auto-generated from scripts/autogen"
    yield " *          in the mlkem-native repository."
    yield " *          Do not modify it directly."
    yield " *)"
    yield ""


def gen_yaml_header():
    yield "# Copyright (c) The mlkem-native project authors"
    yield "# SPDX-License-Identifier: Apache-2.0 OR ISC OR MIT"
    yield ""


def format_content(content):
    p = subprocess.run(
        ["clang-format"], capture_output=True, input=content, text=True, shell=True
    )
    if p.returncode != 0:
        print(p.stderr)
        print(
            f"Failed to auto-format autogenerated code (clang-format return code {p.returncode}). Are you running in a nix shell? See BUILDING.md."
        )
        exit(1)
    return p.stdout


class CondParser:
    """Rudimentary parser for expressions if `#if .. #else ..` directives"""

    def __init__(self):
        c_identifier = pp.common.identifier()
        c_integer_suffix = pp.one_of("U L LU UL LL ULL LLU", caseless=True)
        c_dec_integer = pp.Combine(
            pp.Optional(pp.one_of("+ -"))
            + pp.Word(pp.nums)
            + pp.Optional(c_integer_suffix)
        )
        c_hex_integer = pp.Combine(
            pp.Literal("0x") + pp.Word(pp.hexnums) + pp.Optional(c_integer_suffix)
        )

        self.parser = pp.infix_notation(
            c_identifier | c_hex_integer | c_dec_integer,
            [
                (pp.one_of("!"), 1, pp.opAssoc.RIGHT),
                (pp.one_of("!= == <= >= > <"), 2, pp.opAssoc.LEFT),
                (pp.one_of("&&"), 2, pp.opAssoc.LEFT),
                (pp.one_of("||"), 2, pp.opAssoc.LEFT),
            ],
        )

    @staticmethod
    def connective(res):
        """Extract the top-level connective for the expression"""
        if not isinstance(res, list):
            return None
        elif len(res) == 2:
            # Unary operator (will be "!" in our case)
            return res[0]
        else:
            # Binary operator
            return res[1]

    @staticmethod
    def map_top(f, res):
        """Apply function to arguments of top-level connective"""
        if not isinstance(res, list):
            return res
        else:
            # We expect `f` to do nothing on strings, so it is safe
            # to apply it everywhere, including the connectives.
            return list(map(f, res))

    @staticmethod
    def args(res):
        """Assuming the argument is a binary operation, return all arguments"""
        return res[::2]

    @staticmethod
    def simplify_double_negation(res):
        """Cancel double negations"""
        if CondParser.connective(res) == "!" and CondParser.connective(res[1]) == "!":
            res = res[1][1]
        res = CondParser.map_top(CondParser.simplify_double_negation, res)
        return res

    @staticmethod
    def simplify_not_eq(res):
        """Replace !(x == y) by x != y, and !(x != y) by x == y"""
        if CondParser.connective(res) == "!" and CondParser.connective(res[1]) == "==":
            res = res[1]
            res[1] = "!="
        if CondParser.connective(res) == "!" and CondParser.connective(res[1]) == "!=":
            res = res[1]
            res[1] = "=="
        res = CondParser.map_top(CondParser.simplify_not_eq, res)
        return res

    @staticmethod
    def simplify_neq_chain(res):
        """Check for &&-chains of inequalities followed by an equality
        which implies the inequality. This catches patterns like
        ```
         #if MLKEM_K == 2
         ...
         #elif MLKEM_K == 3
         ...
         #elif MLKEM_K == 4
         ...
         #endif
        ```
        """
        if (
            CondParser.connective(res) == "&&"
            and CondParser.connective(res[-1]) == "=="
        ):
            lhs = res[-1][0]
            rhs = res[-1][2]
            args = []
            for a in CondParser.args(res[:-1]):
                if CondParser.connective(a) == "!=" and a[0] == lhs:
                    args.append(a[2])
                else:
                    args = None
                    break
            if args is None:
                return
            # Check if all args are numerical and different
            if rhs.isdigit() and all(
                map(lambda a: a.isdigit() and int(a) != int(rhs), args)
            ):
                # Success -- just drop all but the final condition
                return res[-1]
        res = CondParser.map_top(CondParser.simplify_neq_chain, res)
        return res

    @staticmethod
    def print_exp(exp, inner=False):
        conn = CondParser.connective(exp)
        if conn is None:
            return exp
        elif conn == "!":
            res = f"!{CondParser.print_exp(exp[1], inner=True)}"
        else:
            padded_conn = f" {conn} "
            res = padded_conn.join(
                map(lambda e: CondParser.print_exp(e, inner=True), CondParser.args(exp))
            )
        if inner is True and conn in ["&&", "||"]:
            res = f"({res})"
        return res

    def simplify_assoc(exp):
        """Check for unnecesary bracketing and remove it"""
        conn = CondParser.connective(exp)
        if conn in ["&&", "||"]:
            args = CondParser.args(exp)
            new_args = []
            for a in args:
                if CondParser.connective(a) == conn:
                    new_args += CondParser.args(a)
                else:
                    new_args.append(a)
            exp = [x for y in map(lambda x: [x, conn], new_args) for x in y][:-1]
        exp = CondParser.map_top(CondParser.simplify_assoc, exp)
        return exp

    def simplify_all(exp):
        exp = CondParser.simplify_double_negation(exp)
        exp = CondParser.simplify_not_eq(exp)
        exp = CondParser.simplify_neq_chain(exp)
        exp = CondParser.simplify_assoc(exp)
        return exp

    def parse_condition(self, exp, simplify=True):
        try:
            exp = self.parser.parseString(exp, parseAll=True).as_list()[0]
        except pp.ParseException:
            print(f"WARNING: Ignoring condition '{exp}' I cannot parse")
            return exp
        if simplify is True:
            exp = CondParser.simplify_all(exp)
        return exp

    def normalize_condition(self, exp):
        return CondParser.print_exp(self.parse_condition(exp))


def adjust_preprocessor_comments_for_filename(content, source_file, show_status=False):
    """Automatically add comments to large `#if ... #else ... #endif`
    blocks indicating the guarding conditions.

    For example, a block

    ```c
      #if FOO
      ...
      #else
      ...
      #endif
    ```

    will be transformed into


    ```c
      #if FOO
      ...
      #else /* FOO */
      ...
      #endif /* !FOO */
    ```

    except when the distance between the preprocessor directives is
    very short, and the annotations would be more harmful than useful.

    ```
    """
    if show_status:
        status_update("if-else", source_file)

    content = content.split("\n")
    new_content = []

    # Stack of `#if` statements. Every entry is a tuple
    # `(conds, line_no, if_or_else, has_children)`, where
    # - `conds` is the list of conditions being tested.
    #   In a normal `#if ... #else ...` braach, this is a singleton list
    #   containing the condition being tested. In a chain of
    #   `#if .. #elif ..` it contains all conditions encountered to this point.
    # - `line_no` is the line where it started
    # - `if_or_else` indicates whether we are in the `#if`
    #   or the `#else` branch (if present)
    # - `force_print` indicates if a comment should be omitted
    if_stack = []

    def merge_escaped_lines(l, i):
        while l.endswith("\\"):
            l = l.removesuffix("\\").rstrip() + content[i + 1].lstrip()
            i = i + 1
        return (l, i)

    def merge_commented_lines(l, i):
        # Not very robust, but good enough
        if not "/*" in l or "*/" in l:
            return (l, i)
        i += 1
        while "*/" not in content[i]:
            l += content[i]
            i += 1

        l += content[i]
        return (l, i)

    def should_print(cur_line_no, conds, line_no, force_print):
        line_threshold = 5
        if force_print is True:
            return True

        if cur_line_no - line_no >= line_threshold:
            return True
        return False

    parser = CondParser()

    def format_condition(cond):
        cond = re.sub(r"defined\(([^)]+)\)", r"\1", cond)
        return parser.normalize_condition(cond)

    def format_conditions(conds, branch):
        prev_conds = list(map(lambda s: f"!({s})", conds[:-1]))
        final_cond = conds[-1]
        if branch is False:
            final_cond = f"!({final_cond})"
        full_cond = "&&".join(prev_conds + [final_cond])
        return format_condition(full_cond)

    def adhoc_format(line):
        # .c and .h files are formatted as a whole
        if not source_file.endswith(".S"):
            return line
        return format_content(line)

    i = 0
    while i < len(content):
        l = content[i].strip()
        # Replace #ifdef by #if defined(...)
        if l.startswith("#ifdef "):
            l = "#if defined(" + l.removeprefix("#ifdef").strip() + ")"
        if l.startswith("#ifndef "):
            l = "#if !defined(" + l.removeprefix("#ifndef").strip() + ")"
        if l.startswith("#if"):
            l, _ = merge_escaped_lines(l, i)
            cond = l.removeprefix("#if")
            if_stack.append(([cond], i, True, False))
            new_content.append(content[i])
        elif l.startswith("#elif"):
            conds, _, _, force_print = if_stack.pop()
            l, _ = merge_escaped_lines(l, i)
            conds.append(l.removeprefix("#elif"))
            if_stack.append((conds, i, True, force_print))
            new_content.append(content[i])
        elif l.startswith("#else"):
            l, i = merge_escaped_lines(l, i)
            _, i = merge_commented_lines(l, i)
            conds, j, branch, force_print = if_stack.pop()
            assert branch is True
            print_else = should_print(i, cond, j, force_print)
            if_stack.append((conds, i, False, print_else))
            if print_else is True:
                cond = format_conditions(conds, True)
                new_content.append(adhoc_format("#else /* " + cond + " */"))
            else:
                new_content.append("#else")
        elif l.startswith("#endif"):
            l, i = merge_escaped_lines(l, i)
            _, i = merge_commented_lines(l, i)
            conds, j, branch, force_print = if_stack.pop()
            print_endif = should_print(i, conds, j, force_print)
            if print_endif is False:
                new_content.append("#endif")
            else:
                cond = format_conditions(conds, branch)
                new_content.append(adhoc_format("#endif /* " + cond + " */"))
        else:
            # Skip over multiline comments -- we don't want to
            # handle `#if ...` inside documentation as this would
            # lead to nested `/* ... */`.
            i_old = i
            _, i = merge_commented_lines(l, i_old)
            new_content += content[i_old : i + 1]
        i += 1

    return "\n".join(new_content)


def gen_preprocessor_comments_for(source_file, dry_run=False):
    with open(source_file, "r") as f:
        content = f.read()
    new_content = adjust_preprocessor_comments_for_filename(
        content, source_file, show_status=True
    )
    update_file(source_file, new_content, dry_run=dry_run)


def gen_preprocessor_comments(dry_run=False):
    files = get_c_source_files() + get_asm_source_files() + get_header_files()
    with ThreadPoolExecutor() as executor:
        results = list(
            executor.map(partial(gen_preprocessor_comments_for, dry_run=dry_run), files)
        )


def update_file(
    filename,
    content,
    dry_run=False,
    force_format=False,
    skip_preprocessor_comments=False,
):

    if force_format is True or filename.endswith((".c", ".h", ".i")):
        if skip_preprocessor_comments is False:
            content = adjust_preprocessor_comments_for_filename(content, filename)
        content = format_content(content)

    if os.path.exists(filename) is True:
        with open(filename, "r") as f:
            current_content = f.read()
    else:
        current_content = None

    if current_content == content:
        return

    if dry_run is False:
        file_updated(filename)
        with open(filename, "w+") as f:
            f.write(content)
    else:
        filename_new = f"{filename}.new"
        error(
            f"Autogenerated file {filename} needs updating. Have you called scripts/autogen?"
        )
        info(f"Writing new version to {filename_new}")
        with open(filename_new, "w") as f:
            f.write(content)
        # If the file exists, print diff between old and new version for debugging
        if current_content != None:
            subprocess.run(["diff", filename, filename_new])
        exit(1)


def bitreverse(i, n):
    r = 0
    for _ in range(n):
        r = 2 * r + (i & 1)
        i >>= 1
    return r


def signed_reduce(a):
    """Return signed canonical representative of a mod b"""
    c = a % modulus
    if c >= modulus / 2:
        c -= modulus
    return c


def gen_c_zetas():
    """Generate source and header file for zeta values used in
    the reference NTT and invNTT"""

    # The zeta values are the powers of the chosen root of unity (17),
    # converted to Montgomery form.

    zeta = []
    for i in range(128):
        zeta.append(signed_reduce(pow(root_of_unity, i, modulus) * montgomery_factor))

    # The source code stores the zeta table in bit reversed form
    yield from (zeta[bitreverse(i, 7)] for i in range(128))


def gen_c_zeta_file(dry_run=False):
    def gen():
        yield from gen_header()
        yield "#include <stdint.h>"
        yield ""
        yield "/*"
        yield " * Table of zeta values used in the reference NTT and inverse NTT."
        yield " * See autogen for details."
        yield " */"
        yield "static MLK_ALIGN const int16_t mlk_zetas[128] = {"
        yield from map(lambda t: str(t) + ",", gen_c_zetas())
        yield "};"
        yield ""

    update_file(
        "mlkem/src/zetas.inc", "\n".join(gen()), dry_run=dry_run, force_format=True
    )


def prepare_root_for_barrett(root):
    """Takes a constant that the code needs to Barrett-multiply with,
    and returns the pair of (a) its signed canonical form, (b) the
    twisted constant used in the high-mul part of the Barrett multiplication."""

    # Signed canonical reduction
    root = signed_reduce(root)

    def round_to_even(t):
        rt = round(t)
        if rt % 2 == 0:
            return rt
        # Make sure to pick a rounding target
        # that's <= 1 away from x in absolute value.
        if rt <= t:
            return rt + 1
        return rt - 1

    root_twisted = round_to_even((root * 2**16) / modulus) // 2
    return root, root_twisted


def gen_aarch64_root_of_unity_for_block(layer, block, inv=False):
    # We are computing a negacyclic NTT; the twiddles needed here is
    # the second half of the twiddles for a cyclic NTT of twice the size.
    # For ease of calculating the roots, layers are numbers 0 through 6
    # in this function.
    log = bitreverse(pow(2, layer) + block, 7)
    if inv is True:
        log = -log
    root, root_twisted = prepare_root_for_barrett(pow(root_of_unity, log, modulus))
    return root, root_twisted


def gen_aarch64_fwd_ntt_zetas_layer12345():
    # Layers 1,2,3 are merged
    yield from gen_aarch64_root_of_unity_for_block(0, 0)
    yield from gen_aarch64_root_of_unity_for_block(1, 0)
    yield from gen_aarch64_root_of_unity_for_block(1, 1)
    yield from gen_aarch64_root_of_unity_for_block(2, 0)
    yield from gen_aarch64_root_of_unity_for_block(2, 1)
    yield from gen_aarch64_root_of_unity_for_block(2, 2)
    yield from gen_aarch64_root_of_unity_for_block(2, 3)
    yield from (0, 0)  # Padding

    # Layers 4,5,6,7 are merged, but we emit roots for 4,5
    # in separate arrays than those for 6,7
    for block in range(8):  # There are 8 blocks in Layer 4
        yield from gen_aarch64_root_of_unity_for_block(3, block)
        yield from gen_aarch64_root_of_unity_for_block(4, 2 * block + 0)
        yield from gen_aarch64_root_of_unity_for_block(4, 2 * block + 1)
        yield from (0, 0)  # Padding


def gen_aarch64_fwd_ntt_zetas_layer67():
    # Layers 4,5,6,7 are merged, but we emit roots for 4,5
    # in separate arrays than those for 6,7
    for block in range(8):

        def double_ith(t, i):
            yield from (t[i], t[i])

        # Ordering of blocks is adjusted to suit the transposed internal
        # presentation of the data
        for i in range(2):
            yield from double_ith(
                gen_aarch64_root_of_unity_for_block(5, 4 * block + 0), i
            )
            yield from double_ith(
                gen_aarch64_root_of_unity_for_block(5, 4 * block + 1), i
            )
            yield from double_ith(
                gen_aarch64_root_of_unity_for_block(5, 4 * block + 2), i
            )
            yield from double_ith(
                gen_aarch64_root_of_unity_for_block(5, 4 * block + 3), i
            )
        for i in range(2):
            yield from double_ith(
                gen_aarch64_root_of_unity_for_block(6, 8 * block + 0), i
            )
            yield from double_ith(
                gen_aarch64_root_of_unity_for_block(6, 8 * block + 2), i
            )
            yield from double_ith(
                gen_aarch64_root_of_unity_for_block(6, 8 * block + 4), i
            )
            yield from double_ith(
                gen_aarch64_root_of_unity_for_block(6, 8 * block + 6), i
            )
        for i in range(2):
            yield from double_ith(
                gen_aarch64_root_of_unity_for_block(6, 8 * block + 1), i
            )
            yield from double_ith(
                gen_aarch64_root_of_unity_for_block(6, 8 * block + 3), i
            )
            yield from double_ith(
                gen_aarch64_root_of_unity_for_block(6, 8 * block + 5), i
            )
            yield from double_ith(
                gen_aarch64_root_of_unity_for_block(6, 8 * block + 7), i
            )


def gen_aarch64_inv_ntt_zetas_layer12345():
    # Layers 4,5,6,7 are merged, but we emit roots for 4,5
    # in separate arrays than those for 6,7
    for block in range(8):  # There are 8 blocks in Layer 4
        yield from gen_aarch64_root_of_unity_for_block(3, block, inv=True)
        yield from gen_aarch64_root_of_unity_for_block(4, 2 * block + 0, inv=True)
        yield from gen_aarch64_root_of_unity_for_block(4, 2 * block + 1, inv=True)
        yield from (0, 0)  # Padding

    # Layers 1,2,3 are merged
    yield from gen_aarch64_root_of_unity_for_block(0, 0, inv=True)
    yield from gen_aarch64_root_of_unity_for_block(1, 0, inv=True)
    yield from gen_aarch64_root_of_unity_for_block(1, 1, inv=True)
    yield from gen_aarch64_root_of_unity_for_block(2, 0, inv=True)
    yield from gen_aarch64_root_of_unity_for_block(2, 1, inv=True)
    yield from gen_aarch64_root_of_unity_for_block(2, 2, inv=True)
    yield from gen_aarch64_root_of_unity_for_block(2, 3, inv=True)
    yield from (0, 0)  # Padding


def gen_aarch64_inv_ntt_zetas_layer67():
    # Layers 4,5,6,7 are merged, but we emit roots for 4,5
    # in separate arrays than those for 6,7
    for block in range(8):

        def double_ith(t, i):
            yield from (t[i], t[i])

        # Ordering of blocks is adjusted to suit the transposed internal
        # presentation of the data
        for i in range(2):
            yield from double_ith(
                gen_aarch64_root_of_unity_for_block(5, 4 * block + 0, inv=True), i
            )
            yield from double_ith(
                gen_aarch64_root_of_unity_for_block(5, 4 * block + 1, inv=True), i
            )
            yield from double_ith(
                gen_aarch64_root_of_unity_for_block(5, 4 * block + 2, inv=True), i
            )
            yield from double_ith(
                gen_aarch64_root_of_unity_for_block(5, 4 * block + 3, inv=True), i
            )
        for i in range(2):
            yield from double_ith(
                gen_aarch64_root_of_unity_for_block(6, 8 * block + 0, inv=True), i
            )
            yield from double_ith(
                gen_aarch64_root_of_unity_for_block(6, 8 * block + 2, inv=True), i
            )
            yield from double_ith(
                gen_aarch64_root_of_unity_for_block(6, 8 * block + 4, inv=True), i
            )
            yield from double_ith(
                gen_aarch64_root_of_unity_for_block(6, 8 * block + 6, inv=True), i
            )
        for i in range(2):
            yield from double_ith(
                gen_aarch64_root_of_unity_for_block(6, 8 * block + 1, inv=True), i
            )
            yield from double_ith(
                gen_aarch64_root_of_unity_for_block(6, 8 * block + 3, inv=True), i
            )
            yield from double_ith(
                gen_aarch64_root_of_unity_for_block(6, 8 * block + 5, inv=True), i
            )
            yield from double_ith(
                gen_aarch64_root_of_unity_for_block(6, 8 * block + 7, inv=True), i
            )


def gen_aarch64_mulcache_twiddles():
    for idx in range(0, 128):
        root = pow(root_of_unity, 2 * bitreverse(idx, 7) + 1, modulus)
        yield prepare_root_for_barrett(root)[0]


def gen_aarch64_mulcache_twiddles_twisted():
    for idx in range(0, 128):
        root = pow(root_of_unity, 2 * bitreverse(idx, 7) + 1, modulus)
        yield prepare_root_for_barrett(root)[1]


def print_hol_light_array(g, as_int=True, entries_per_line=8, pad=0):

    # Format of integer list entries, including `;` separator:
    # - Positive numbers: &42;
    # - Negative numbers: -- &42;
    # If as_int is false, we omit `&` and emit constant as numerals.
    def format_hol_light_int(n):
        prefix = ""
        if n < 0:
            prefix = "-- "
            n = -n
        c = "&" if as_int is True else ""
        return f"{prefix}{c}{n:>{pad}};"

    l = list(map(format_hol_light_int, g))
    # Remove `;` from end of last entry
    l[-1] = l[-1][:-1]

    for i in range(0, len(l), entries_per_line):
        yield "  " + " ".join(l[i : i + entries_per_line])


def gen_aarch64_hol_light_zeta_file(dry_run=False):
    def gen():
        yield from gen_hol_light_header()
        yield "(*"
        yield " * Table of zeta values used in the AArch64 NTTs"
        yield " * See autogen for details."
        yield " *)"
        yield ""
        yield "let ntt_zetas_layer12345 = define `ntt_zetas_layer12345:int list = ["
        yield from print_hol_light_array(gen_aarch64_fwd_ntt_zetas_layer12345())
        yield "]`;;"
        yield ""
        yield "let ntt_zetas_layer67 = define `ntt_zetas_layer67:int list = ["
        yield from print_hol_light_array(gen_aarch64_fwd_ntt_zetas_layer67())
        yield "]`;;"
        yield ""
        yield "let intt_zetas_layer12345 = define `intt_zetas_layer12345:int list = ["
        yield from print_hol_light_array(gen_aarch64_inv_ntt_zetas_layer12345())
        yield "]`;;"
        yield ""
        yield "let intt_zetas_layer67 = define `intt_zetas_layer67:int list = ["
        yield from print_hol_light_array(gen_aarch64_inv_ntt_zetas_layer67())
        yield "]`;;"
        yield ""
        yield "let mulcache_zetas = define `mulcache_zetas:int list = ["
        yield from print_hol_light_array(gen_aarch64_mulcache_twiddles())
        yield "]`;;"
        yield ""
        yield ""
        yield "let mulcache_zetas_twisted = define `mulcache_zetas_twisted:int list = ["
        yield from print_hol_light_array(gen_aarch64_mulcache_twiddles_twisted())
        yield "]`;;"
        yield ""

    update_file(
        "proofs/hol_light/arm/proofs/mlkem_zetas.ml",
        "\n".join(gen()),
        dry_run=dry_run,
    )


def gen_aarch64_zeta_file(dry_run=False):
    def gen():
        yield from gen_header()
        yield '#include "../../../common.h"'
        yield ""
        yield "#if defined(MLK_ARITH_BACKEND_AARCH64) && \\"
        yield "    !defined(MLK_CONFIG_MULTILEVEL_NO_SHARED)"
        yield ""
        yield "#include <stdint.h>"
        yield '#include "arith_native_aarch64.h"'
        yield ""
        yield "/*"
        yield " * Table of zeta values used in the AArch64 forward NTT"
        yield " * See autogen for details."
        yield " */"
        yield "MLK_ALIGN const int16_t mlk_aarch64_ntt_zetas_layer12345[] = {"
        yield from map(lambda t: str(t) + ",", gen_aarch64_fwd_ntt_zetas_layer12345())
        yield "};"
        yield ""
        yield "MLK_ALIGN const int16_t mlk_aarch64_ntt_zetas_layer67[] = {"
        yield from map(lambda t: str(t) + ",", gen_aarch64_fwd_ntt_zetas_layer67())
        yield "};"
        yield ""
        yield "MLK_ALIGN const int16_t mlk_aarch64_invntt_zetas_layer12345[] = {"
        yield from map(lambda t: str(t) + ",", gen_aarch64_inv_ntt_zetas_layer12345())
        yield "};"
        yield ""
        yield "MLK_ALIGN const int16_t mlk_aarch64_invntt_zetas_layer67[] = {"
        yield from map(lambda t: str(t) + ",", gen_aarch64_inv_ntt_zetas_layer67())
        yield "};"
        yield ""
        yield "MLK_ALIGN const int16_t mlk_aarch64_zetas_mulcache_native[] = {"
        yield from map(lambda t: str(t) + ",", gen_aarch64_mulcache_twiddles())
        yield "};"
        yield ""
        yield "MLK_ALIGN const int16_t mlk_aarch64_zetas_mulcache_twisted_native[] = {"
        yield from map(lambda t: str(t) + ",", gen_aarch64_mulcache_twiddles_twisted())
        yield "};"
        yield ""
        yield "#else"
        yield ""
        yield "MLK_EMPTY_CU(aarch64_zetas)"
        yield ""
        yield "#endif"
        yield ""

    update_file(
        "dev/aarch64_opt/src/aarch64_zetas.c",
        "\n".join(gen()),
        dry_run=dry_run,
    )

    update_file(
        "dev/aarch64_clean/src/aarch64_zetas.c",
        "\n".join(gen()),
        dry_run=dry_run,
    )


def gen_rej_uniform_table_rows():
    # The index into the lookup table is an 8-bit bitmap, i.e. a number 0..255.
    # Conceptually, the table entry at index i is a vector of 8 16-bit values, of
    # which only the first popcount(i) are set; those are the indices of the set-bits
    # in i. Concretely, we store each 16-bit index as consecutive 8-bit indices.
    def get_set_bits_idxs(i):
        bits = list(map(int, format(i, "08b")))
        bits.reverse()
        return [bit_idx for bit_idx in range(8) if bits[bit_idx] == 1]

    for i in range(256):
        idxs = get_set_bits_idxs(i)
        # Replace each index by two consecutive indices
        idxs = [j for i in idxs for j in [2 * i, 2 * i + 1]]
        # Pad by -1
        idxs = idxs + [255] * (16 - len(idxs))
        yield idxs


def gen_aarch64_hol_light_rej_uniform_table(dry_run=False):
    def gen():
        yield from gen_hol_light_header()
        yield "(*"
        yield " * Constant table values used in the AArch64 rejection sampling."
        yield " * See autogen for details."
        yield " *)"
        yield ""
        yield "let mlkem_rej_uniform_table = (REWRITE_RULE[MAP] o define)"
        yield "  `mlkem_rej_uniform_table:byte list = MAP word ["
        data = [i for idxs in gen_rej_uniform_table_rows() for i in idxs]
        yield from print_hol_light_array(data, as_int=False, entries_per_line=16, pad=3)
        yield "]`;;"
        yield ""

    update_file(
        "proofs/hol_light/arm/proofs/mlkem_rej_uniform_table.ml",
        "\n".join(gen()),
        dry_run=dry_run,
    )


def gen_aarch64_rej_uniform_table(dry_run=False):
    def gen():
        yield from gen_header()
        yield '#include "../../../common.h"'
        yield ""
        yield "#if defined(MLK_ARITH_BACKEND_AARCH64) && \\"
        yield "    !defined(MLK_CONFIG_MULTILEVEL_NO_SHARED)"
        yield ""
        yield "#include <stdint.h>"
        yield '#include "arith_native_aarch64.h"'
        yield ""
        yield "/*"
        yield " * Lookup table used by rejection sampling of the public matrix."
        yield " * See autogen for details."
        yield " */"
        yield "MLK_ALIGN const uint8_t mlk_rej_uniform_table[] = {"
        for i, idxs in enumerate(gen_rej_uniform_table_rows()):
            yield ",".join(map(str, idxs)) + f" /* {i} */,"
        yield "};"
        yield ""
        yield "#else"
        yield ""
        yield "MLK_EMPTY_CU(aarch64_rej_uniform_table)"
        yield ""
        yield "#endif"
        yield ""

    update_file(
        "dev/aarch64_opt/src/rej_uniform_table.c",
        "\n".join(gen()),
        dry_run=dry_run,
    )

    update_file(
        "dev/aarch64_clean/src/rej_uniform_table.c",
        "\n".join(gen()),
        dry_run=dry_run,
    )


def gen_avx2_rej_uniform_table(dry_run=False):
    def gen():
        yield from gen_header()
        yield '#include "../../../common.h"'
        yield ""
        yield "#if defined(MLK_ARITH_BACKEND_X86_64_DEFAULT) && \\"
        yield "    !defined(MLK_CONFIG_MULTILEVEL_NO_SHARED)"
        yield ""
        yield "#include <stdint.h>"
        yield '#include "arith_native_x86_64.h"'
        yield ""
        yield "/*"
        yield " * Lookup table used by rejection sampling of the public matrix."
        yield " * See autogen for details."
        yield " */"
        yield "MLK_ALIGN const uint8_t mlk_rej_uniform_table[] = {"
        for i, idxs in enumerate(gen_rej_uniform_table_rows()):
            yield ",".join(map(str, idxs)) + f" /* {i} */,"
        yield "};"
        yield ""
        yield "#else"
        yield ""
        yield "MLK_EMPTY_CU(avx2_rej_uniform_table)"
        yield ""
        yield "#endif"
        yield ""

    update_file(
        "dev/x86_64/src/rej_uniform_table.c",
        "\n".join(gen()),
        dry_run=dry_run,
    )


def signed_reduce_u16(x):
    x = x % 2**16
    if x >= 2**15:
        x -= 2**16
    return x


def prepare_root_for_montmul(root):
    """Takes a constant that the code needs to Montgomery-multiply with,
    and returns the pair of (a) the signed canonical representative of its
    Montgomery form, (b) the twisted constant used in the low-mul part of
    the Montgomery multiplication."""

    # Convert to Montgomery form and pick canonical signed representative
    root = signed_reduce(root * montgomery_factor)
    root_twisted = signed_reduce_u16(root * pow(modulus, -1, 2**16))
    return root, root_twisted


def gen_avx2_root_of_unity_for_block(layer, block, inv=False):
    # We are computing a negacyclic NTT; the twiddles needed here is
    # the second half of the twiddles for a cyclic NTT of twice the size.
    log = bitreverse(pow(2, layer) + block, 7)
    if inv is True:
        log = -log
    root, root_twisted = prepare_root_for_montmul(pow(root_of_unity, log, modulus))
    return root, root_twisted


def gen_avx2_fwd_ntt_zetas():

    def gen_twiddles(layer, block, repeat):
        """Generates twisted twiddle, then twiddle, for given layer and block.
        Repeat both the given number of times."""
        root, root_twisted = gen_avx2_root_of_unity_for_block(layer, block)
        return [root] * repeat, [root_twisted] * repeat

    def gen_twiddles_many(layer, block_base, block_offsets, repeat):
        """Generates twisted twiddles, then twiddles, of each (layer, block_base + i)
        pair for i in block_offsets. Each twiddle is repeated `repeat` times."""
        root_pairs = list(
            map(lambda x: gen_twiddles(layer, block_base + x, repeat), block_offsets)
        )
        yield from (r for l in root_pairs for r in l[1])
        yield from (r for l in root_pairs for r in l[0])

    # Layers 1 twiddle
    yield from gen_twiddles_many(0, 0, range(1), 4)
    # Padding so that the subsequent twiddles are 16-byte aligned
    yield from [0] * 8

    # Layer 2-7 twiddles, separated by whether they belong to the upper or lower half
    for i in range(2):
        yield from gen_twiddles_many(1, i * (2**0), range(1), 16)
        yield from gen_twiddles_many(2, i * (2**1), range(2), 8)
        yield from gen_twiddles_many(3, i * (2**2), range(4), 4)
        yield from gen_twiddles_many(4, i * (2**3), range(8), 2)
        yield from gen_twiddles_many(5, i * (2**4), range(16), 1)
        yield from gen_twiddles_many(6, i * (2**5), range(0, 32, 2), 1)
        yield from gen_twiddles_many(6, i * (2**5), range(1, 32, 2), 1)


def gen_avx2_mulcache_twiddles():
    for i in range(2):
        for idx in range(16):
            root, root_twisted = prepare_root_for_montmul(
                pow(root_of_unity, bitreverse(64 + 32 * i + 2 * idx, 7), modulus)
            )
            yield root

        for idx in range(16):
            root, root_twisted = prepare_root_for_montmul(
                pow(root_of_unity, bitreverse(64 + 32 * i + 2 * idx + 1, 7), modulus)
            )
            yield root

    for i in range(2):
        for idx in range(16):
            root, root_twisted = prepare_root_for_montmul(
                pow(root_of_unity, bitreverse(64 + 32 * i + 2 * idx, 7), modulus)
            )
            yield root_twisted

        for idx in range(16):
            root, root_twisted = prepare_root_for_montmul(
                pow(root_of_unity, bitreverse(64 + 32 * i + 2 * idx + 1, 7), modulus)
            )
            yield root_twisted


def gen_avx2_zeta_file(dry_run=False):
    def gen():
        yield from gen_header()
        yield "/*"
        yield " * Table of zeta values used in the AVX2 NTTs"
        yield " * See autogen for details."
        yield " */"
        yield ""
        yield from map(lambda t: str(t) + ",", gen_avx2_fwd_ntt_zetas())
        yield ""

    update_file(
        "mlkem/src/native/x86_64/src/x86_64_zetas.i", "\n".join(gen()), dry_run=dry_run
    )


def gen_avx2_mulcache_twiddles_file(dry_run=False):
    def gen():
        yield from gen_header()
        yield "/*"
        yield " * Table of twiddle values used in the AVX2 mulcache"
        yield " * See autogen for details."
        yield " */"
        yield ""
        yield from map(lambda t: str(t) + ",", gen_avx2_mulcache_twiddles())
        yield ""

    update_file(
        "dev/x86_64/src/x86_64_mulcache_twiddles.i", "\n".join(gen()), dry_run=dry_run
    )


def get_c_source_files(main_only=False, core_only=False, strip_mlkem=False):
    if main_only is True:
        return get_files("mlkem/src/**/*.c", strip_mlkem=strip_mlkem)
    elif core_only is True:
        return get_files("mlkem/src/**/*.c", strip_mlkem=strip_mlkem) + get_files(
            "dev/**/*.c"
        )
    else:
        return get_files("**/*.c", strip_mlkem=strip_mlkem)


def get_asm_source_files(main_only=False, core_only=False, strip_mlkem=False):
    if main_only is True:
        return get_files("mlkem/src/**/*.S", strip_mlkem=strip_mlkem)
    elif core_only is True:
        return get_files("mlkem/src/**/*.S", strip_mlkem=strip_mlkem) + get_files(
            "dev/**/*.S", strip_mlkem=strip_mlkem
        )
    else:
        return get_files("**/*.S", strip_mlkem=strip_mlkem)


def get_header_files(main_only=False, core_only=False):
    if main_only is True:
        return get_files("mlkem/*.h") + get_files("mlkem/src/**/*.h")
    elif core_only is True:
        return (
            get_files("mlkem/*.h")
            + get_files("mlkem/src/**/*.h")
            + get_files("dev/**/*.h")
            + get_files("integration/**/*.h")
        )
    else:
        return get_files("**/*.h")


def get_markdown_files(main_only=False):
    return get_files("**/*.md")


def get_files(pattern, strip_mlkem=False):
    return [
        str(p).removeprefix("mlkem/") if strip_mlkem else str(p)
        for p in pathlib.Path().glob(pattern)
        if p.is_file() and not p.is_symlink()
    ]


def get_all_files():
    # All git-tracked files, including symlinks
    r = subprocess.run(["git", "ls-files"], capture_output=True, text=True)
    assert r.returncode == 0
    files = r.stdout.split("\n")
    files = filter(lambda s: s != "" and pathlib.Path(s).is_symlink() is False, files)
    return files


def get_defines_from_file(c):
    with open(c, "r") as f:
        for l in f.read().split("\n"):
            if l.lstrip().startswith("#define "):
                yield (
                    c,
                    l.lstrip()
                    .removeprefix("#define ")
                    .split(" ")[0]
                    .split("(")[0]
                    .replace("'", ""),
                )


def get_defines(all=False):
    if all is False:
        files = get_header_files(main_only=True)
    else:
        files = get_header_files() + get_c_source_files() + get_asm_source_files()
    for c in files:
        yield from get_defines_from_file(c)


def get_checked_defines():
    allow_list = [("__contract__", "cbmc.h"), ("__loop__", "cbmc.h")]

    def is_allowed(d, c):
        for d0, c0 in allow_list:
            if c.endswith(c0) is True and d0 == d:
                return True
        return False

    for c, d in get_defines():
        if d.startswith("_") and is_allowed(d, c) is False:
            raise Exception(
                f"{d} from {c}: starts with an underscore, which is not allowed for mlkem-native macros. "
                f"If this is an mlkem-native specific macro, please pick a different name. "
                f"If this is an external macro, it likely needs removing from `gen_monolithic_undef_all_core()` in `scripts/autogen` -- check this!"
            )
        yield (c, d)


def gen_monolithic_undef_all_core(filt=None, desc=""):

    if filt is None:
        filt = lambda c: True

    if desc != "":
        yield "/*"
        yield f" * Undefine macros from {desc}"
        yield " */"

    defines = list(set(get_checked_defines()))
    defines.sort()

    last_filename = None
    for filename, d in defines:
        if filt(filename) is False:
            continue
        if last_filename != filename:
            yield f"/* {filename} */"
            last_filename = filename
        yield f"#undef {d}"


def native(c):
    return "/native/" in c


def fips202(c):
    return "/fips202/" in c


def aarch64(c):
    return "/aarch64/" in c


def x86_64(c):
    return "/x86_64/" in c


def riscv64(c):
    return "/riscv64/" in c


def native_fips202(c):
    return native(c) and fips202(c)


def native_arith(c):
    return native(c) and not fips202(c)


def native_fips202_aarch64(c):
    return native_fips202(c) and aarch64(c)


def native_fips202_x86_64(c):
    return native_fips202(c) and x86_64(c)


def native_fips202_core(c):
    return (
        native_fips202(c)
        and not native_fips202_x86_64(c)
        and not native_fips202_aarch64(c)
    )


def native_arith_aarch64(c):
    return native_arith(c) and aarch64(c)


def native_arith_x86_64(c):
    return native_arith(c) and x86_64(c)


def native_arith_riscv64(c):
    return native_arith(c) and riscv64(c)


def native_arith_core(c):
    return (
        native_arith(c)
        and not native_arith_x86_64(c)
        and not native_arith_aarch64(c)
        and not native_arith_riscv64(c)
    )


# List of level-specific source files
# All other files only need including and building once
# in multi-level build.
def k_specific(c):
    k_specific_sources = [
        "mlkem_native.h",
        "params.h",
        # Deliberately omit config.h, which is not #undef'ed
        "common.h",
        "indcpa.c",
        "indcpa.h",
        "kem.c",
        "kem.h",
        "poly_k.c",
        "poly_k.h",
    ]
    for f in k_specific_sources:
        if c.endswith(f):
            return True
    return False


def k_generic(c):
    return not k_specific(c) and c != "mlkem/src/config.h"


def gen_macro_undefs(extra_notes=None):
    if extra_notes is None:
        extra_notes = []

    yield "/* Macro #undef's"
    yield " *"
    yield " * The following undefines macros from headers"
    yield " * included by the source files imported above."
    yield " *"
    yield " * This is to allow building and linking multiple builds"
    yield " * of mlkem-native for varying parameter sets through concatenation"
    yield " * of this file, as if the files had been compiled separately."
    yield " * If this is not relevant to you, you may remove the following."
    for e in extra_notes:
        yield f" * {e}"
    yield " */"
    yield ""
    yield from gen_monolithic_undef_all_core(
        filt=k_specific, desc="MLK_CONFIG_PARAMETER_SET-specific files"
    )
    yield ""
    yield "#if !defined(MLK_CONFIG_MONOBUILD_KEEP_SHARED_HEADERS)"
    yield from gen_monolithic_undef_all_core(
        filt=lambda c: not native(c)
        and k_generic(c)
        and not fips202(c)
        and "cbmc.h" not in c,
        desc="MLK_CONFIG_PARAMETER_SET-generic files",
    )
    # Handle cbmc.h manually -- most #define's therein are only defined when CBMC is set
    # and need not be #undef'ed. In fact, #undef'ing them is risky since their names may
    # well already be occupied.
    yield "/* mlkem/src/cbmc.h */"
    yield "#undef MLK_CBMC_H"
    yield "#undef __contract__"
    yield "#undef __loop__"
    yield ""
    yield "#if !defined(MLK_CONFIG_FIPS202_CUSTOM_HEADER)"
    yield from gen_monolithic_undef_all_core(
        filt=lambda c: not native(c) and k_generic(c) and fips202(c),
        desc="FIPS-202 files",
    )
    yield "#endif"
    yield ""
    yield "#if defined(MLK_CONFIG_USE_NATIVE_BACKEND_FIPS202)"
    yield from gen_monolithic_undef_all_core(filt=native_fips202_core, desc="")
    yield "#if defined(MLK_SYS_AARCH64)"
    yield from gen_monolithic_undef_all_core(
        filt=native_fips202_aarch64, desc="native code (FIPS202, AArch64)"
    )
    yield "#endif"
    yield "#if defined(MLK_SYS_X86_64)"
    yield from gen_monolithic_undef_all_core(
        filt=native_fips202_x86_64, desc="native code (FIPS202, x86_64)"
    )
    yield "#endif"
    yield "#endif"
    yield "#if defined(MLK_CONFIG_USE_NATIVE_BACKEND_ARITH)"
    yield from gen_monolithic_undef_all_core(filt=native_arith_core, desc="")
    yield "#if defined(MLK_SYS_AARCH64)"
    yield from gen_monolithic_undef_all_core(
        filt=native_arith_aarch64, desc="native code (Arith, AArch64)"
    )
    yield "#endif"
    yield "#if defined(MLK_SYS_X86_64)"
    yield from gen_monolithic_undef_all_core(
        filt=native_arith_x86_64, desc="native code (Arith, X86_64)"
    )
    yield "#endif"
    yield "#endif"
    yield "#endif"
    yield ""


def gen_monolithic_source_file(dry_run=False):

    def gen():
        c_sources = get_c_source_files(main_only=True, strip_mlkem=True)
        yield from gen_header()

        yield "/******************************************************************************"
        yield " *"
        yield " * Single compilation unit (SCU) for fixed-level build of mlkem-native"
        yield " *"
        yield " * This compilation unit bundles together all source files for a build"
        yield " * of mlkem-native for a fixed security level (MLKEM-512/768/1024)."
        yield " *"
        yield " * # API"
        yield " *"
        yield " * The API exposed by this file is described in mlkem_native.h."
        yield " *"
        yield " * # Multi-level build"
        yield " *"
        yield " * If you want an SCU build of mlkem-native with support for multiple security"
        yield " * levels, you need to include this file multiple times, and set"
        yield " * MLK_CONFIG_MULTILEVEL_WITH_SHARED and MLK_CONFIG_MULTILEVEL_NO_SHARED"
        yield " * appropriately. This is exemplified in examples/monolithic_build_multilevel"
        yield " * and examples/monolithic_build_multilevel_native."
        yield " *"
        yield " * # Configuration"
        yield " *"
        yield " * The following options from the mlkem-native configuration are relevant:"
        yield " *"
        yield " * - MLK_CONFIG_FIPS202_CUSTOM_HEADER"
        yield " *   Set this option if you use a custom FIPS202 implementation."
        yield " *"
        yield " * - MLK_CONFIG_USE_NATIVE_BACKEND_ARITH"
        yield " *   Set this option if you want to include the native arithmetic backends"
        yield " *   in your build."
        yield " *"
        yield " * - MLK_CONFIG_USE_NATIVE_BACKEND_FIPS202"
        yield " *   Set this option if you want to include the native FIPS202 backends"
        yield " *   in your build."
        yield " *"
        yield " * - MLK_CONFIG_MONOBUILD_KEEP_SHARED_HEADERS"
        yield " *   Set this option if you want to keep the directives defined in"
        yield " *   level-independent headers. This is needed for a multi-level build."
        yield " */"
        yield " "
        yield " /* If parts of the mlkem-native source tree are not used,"
        yield " * consider reducing this header via `unifdef`."
        yield " *"
        yield " * Example:"
        yield " * ```bash"
        yield " * unifdef -UMLK_CONFIG_USE_NATIVE_BACKEND_ARITH mlkem_native.c"
        yield " * ```"
        yield " */"
        yield ""
        yield '#include "src/common.h"'
        yield ""
        for c in filter(lambda c: not native(c) and not fips202(c), c_sources):
            yield f'#include "{c}"'
        yield ""
        yield "#if !defined(MLK_CONFIG_FIPS202_CUSTOM_HEADER)"
        for c in filter(lambda c: not native(c) and fips202(c), c_sources):
            yield f'#include "{c}"'
        yield "#endif"
        yield ""
        yield "#if defined(MLK_CONFIG_USE_NATIVE_BACKEND_ARITH)"
        yield "#if defined(MLK_SYS_AARCH64)"
        for c in filter(native_arith_aarch64, c_sources):
            yield f'#include "{c}"'
        yield "#endif"
        yield "#if defined(MLK_SYS_X86_64)"
        for c in filter(native_arith_x86_64, c_sources):
            yield f'#include "{c}"'
        yield "#endif"
        yield "#endif"
        yield ""
        yield "#if defined(MLK_CONFIG_USE_NATIVE_BACKEND_FIPS202)"
        yield "#if defined(MLK_SYS_AARCH64)"
        for c in filter(native_fips202_aarch64, c_sources):
            yield f'#include "{c}"'
        yield "#endif"
        yield "#if defined(MLK_SYS_X86_64)"
        for c in filter(native_fips202_x86_64, c_sources):
            yield f'#include "{c}"'
        yield "#endif"
        yield "#endif"
        yield ""
        yield from gen_macro_undefs()

    update_file(
        "mlkem/mlkem_native.c",
        "\n".join(gen()),
        dry_run=dry_run,
    )


def gen_monolithic_asm_file(dry_run=False):

    def gen():
        asm_sources = get_asm_source_files(main_only=True)
        yield from gen_header()
        yield "/******************************************************************************"
        yield " *"
        yield " * Single assembly unit for fixed-level build of mlkem-native"
        yield " *"
        yield " * This assembly unit bundles together all assembly files for a build"
        yield " * of mlkem-native for a fixed security level (MLKEM-512/768/1024)."
        yield " *"
        yield " * # Multi-level build"
        yield " *"
        yield " * If you want an SCU build of mlkem-native with support for multiple security"
        yield " * levels, you should include this file once with MLK_CONFIG_MULTILEVEL_WITH_SHARED set."
        yield " *"
        yield " * (You could also follow the same pattern as for mlkem_native_monobuild.c"
        yield " *  and include it for every level, setting MLK_CONFIG_MULTILEVEL_NO_SHARED"
        yield " *  for all but one. For builds with MLK_CONFIG_MULTILEVEL_NO_SHARED, this"
        yield " *  file will then be ignored.)"
        yield " *"
        yield " * # Configuration"
        yield " *"
        yield " * The following options from the mlkem-native configuration are relevant:"
        yield " *"
        yield " * - MLK_CONFIG_FIPS202_CUSTOM_HEADER"
        yield " *   Set this option if you use a custom FIPS202 implementation."
        yield " *"
        yield " * - MLK_CONFIG_USE_NATIVE_BACKEND_ARITH"
        yield " *   Set this option if you want to include the native arithmetic backends"
        yield " *   in your build."
        yield " *"
        yield " * - MLK_CONFIG_USE_NATIVE_BACKEND_FIPS202"
        yield " *   Set this option if you want to include the native FIPS202 backends"
        yield " *   in your build."
        yield " *"
        yield " * - MLK_CONFIG_MONOBUILD_KEEP_SHARED_HEADERS"
        yield " *   Set this option if you want to keep the directives defined in"
        yield " *   level-independent headers. This is needed for a multi-level build."
        yield " */"
        yield ""
        yield "/* If parts of the mlkem-native source tree are not used,"
        yield " * consider reducing this header via `unifdef`."
        yield " *"
        yield " * Example:"
        yield " * ```bash"
        yield " * unifdef -UMLK_CONFIG_USE_NATIVE_BACKEND_ARITH mlkem_native.S"
        yield " * ```"
        yield " */"
        yield ""
        yield '#include "src/common.h"'
        yield ""
        yield "#if defined(MLK_CONFIG_USE_NATIVE_BACKEND_ARITH)"
        yield "#if defined(MLK_SYS_AARCH64)"
        for c in filter(native_arith_aarch64, asm_sources):
            yield f'#include "{c}"'
        yield "#endif"
        yield "#if defined(MLK_SYS_X86_64)"
        for c in filter(native_arith_x86_64, asm_sources):
            yield f'#include "{c}"'
        yield "#endif"
        yield "#if defined(MLK_SYS_RISCV64)"
        for c in filter(native_arith_riscv64, asm_sources):
            yield f'#include "{c}"'
        yield "#endif"
        yield "#endif"
        yield ""
        yield "#if defined(MLK_CONFIG_USE_NATIVE_BACKEND_FIPS202)"
        yield "#if defined(MLK_SYS_AARCH64)"
        for c in filter(native_fips202_aarch64, asm_sources):
            yield f'#include "{c}"'
        yield "#endif"
        yield "#if defined(MLK_SYS_X86_64)"
        for c in filter(native_fips202_x86_64, asm_sources):
            yield f'#include "{c}"'
        yield "#endif"
        yield "#endif"
        yield ""
        # We generate #undef's for all headers, even though most are not
        # included by the assembly files. This does not harm, and avoids
        # having to trace which headers are being pulled in from common.h
        # included from the assembly files.
        yield ""
        extra = [
            "",
            "NOTE: This is not needed for the assembly SCU since, at present,",
            "there is no need to include it multiple times.",
            "We keep it for uniformity with mlkem_native.c only.",
            "",
            "NOTE: To avoid having to distinguish between which headers are included",
            "from the assembly files, we #undef the same set of directives",
            "as in mlkem_native.c",
        ]
        yield from gen_macro_undefs(extra_notes=extra)

    update_file(
        "mlkem/mlkem_native.S",
        "\n".join(gen()),
        dry_run=dry_run,
        force_format=True,
    )


def get_config_options():
    with open("mlkem/src/config.h", "r") as f:
        content = f.read()
    config_pattern = r"Name:\s*(MLK_CONFIG_\w+)"

    configs = re.findall(config_pattern, content)

    configs += [
        "MLK_FORCE_AARCH64",
        "MLK_FORCE_AARCH64_EB",
        "MLK_FORCE_X86_64",
        "MLK_FORCE_PPC64LE",
        "MLK_FORCE_RISCV64",
        "MLK_FORCE_RISCV32",
        "MLK_SYS_AARCH64_SLOW_BARREL_SHIFTER",
        "MLKEM_DEBUG",  # TODO: Rename?
        "MLK_BREAK_PCT",  # Use in PCT breakage test]
        "MLK_CONFIG_NO_ASM_VALUE_BARRIER",  # TODO: Add to config?
        "MLK_CHECK_APIS",
        "MLK_CONFIG_API_XXX",
        "MLK_USE_NATIVE_XXX",
    ]

    return configs


def check_macro_typos_in_file(filename, macro_check):
    """Checks for typos in MLK_XXX and MLKEM_XXX identifiers."""
    status_update("check-macros", filename)
    with open(filename, "r") as f:
        content = f.read()

    pattern = r"[^_]((?:MLK_|MLKEM_)\w+)(.*)$"
    for m in re.finditer(pattern, content, flags=re.M):
        txt = m.group(1)
        rest = m.group(2)
        if macro_check(txt, rest, filename) is False:
            line_no = content[: m.start()].count("\n") + 1
            raise Exception(
                f"Likely typo {txt} in {filename}:{line_no}? Not a defined macro."
            )


def get_syscaps():
    return ["MLK_SYS_CAP_AVX2", "MLK_SYS_CAP_SHA3", "MLK_SYS_CAP_DUMMY"]


def check_macro_typos():
    files = get_all_files()
    configs = get_config_options()
    syscaps = get_syscaps()

    macros = set(map(lambda t: t[1], get_defines(all=True)))
    # Add configuration options to the list of allows macro names
    macros.update(get_config_options())

    def macro_check(m, rest, filename):
        if m in macros:
            return True

        is_autogen = filename == "scripts/autogen"

        # Exclude system capabilities, which are enum values
        if m in syscaps:
            return True

        #
        # Register some file-specific exceptions
        #

        # 1. Makefiles use MLK_SOURCE_XXX to list source files
        if is_autogen or filename.endswith("/Makefile"):
            if m.startswith("MLK_SOURCE") or m.startswith("MLK_OBJ"):
                return True

        # 2. libOQS specific identifier
        if is_autogen or filename.startswith("integration/liboqs"):
            if m.startswith("MLKEM_NATIVE_MLKEM") or m in ["MLKEM_DIR"]:
                return True

        # 3. Exclude HOL-Light proof scripts
        if is_autogen or filename.startswith("proofs/hol_light"):
            if filename.endswith(".ml"):
                return True

        # 4. Exclude regexp patterns in `autogen`
        if is_autogen:
            if rest.startswith("\\") or m in ["MLK_XXX", "MLK_SOURCE_XXX"]:
                return True

        # 5. AWS-LC importer patch
        if is_autogen or filename == "integration/awslc/awslc.patch":
            return True

        return False

    with ThreadPoolExecutor() as executor:
        results = list(
            executor.map(
                partial(check_macro_typos_in_file, macro_check=macro_check), files
            )
        )


def check_asm_register_aliases_for_file(filename):
    """Checks that `filename` has no mismatching or dangling register aliases"""
    status_update("reg alises", filename)

    def get_alias_def(l):
        s = list(filter(lambda s: s != "", l.strip().split(" ")))
        if len(s) < 3 or s[1] != ".req":
            return None
        return s[0]

    def get_alias_undef(l):
        if l.strip().startswith(".unreq") is False:
            return None
        return list(filter(lambda s: s != "", l.strip().split(" ")))[1]

    with open(filename, "r") as f:
        content = f.read()
    aliases = {}
    for i, l in enumerate(content.split("\n")):
        alias_def = get_alias_def(l)
        alias_undef = get_alias_undef(l)
        if alias_def is not None:
            if alias_def in aliases.keys():
                raise Exception(
                    f"Invalid assembly file {filename}: Duplicate .req directive for {alias_def} at line {i}"
                )
            aliases[alias_def] = i
        elif alias_undef is not None:
            if alias_undef not in aliases.keys():
                raise Exception(
                    f"Invalid assembly file {filename}: .unreq without prior .req for {alias_undef} at line {i}"
                )
            del aliases[alias_undef]

    if len(aliases) > 0:
        fixup_suggestion = [
            "/****************** REGISTER DEALLOCATIONS *******************/"
        ]
        dangling = list(aliases.items())
        # Sort by line number of .req
        dangling.sort(key=lambda s: s[1])

        for a, _ in dangling:
            fixup_suggestion.append(f"    .unreq {a}")
        fixup_suggestion.append("")
        fixup_suggestion = "\n".join(fixup_suggestion)

        raise Exception(
            f"Invalid assembly file {filename}: Dangling .req directives {aliases}.\n\nTry adding this?\n\n{fixup_suggestion}"
        )


def check_asm_register_aliases():
    for asm_file in get_asm_source_files():
        check_asm_register_aliases_for_file(asm_file)


def check_asm_loop_labels_for_file(filename):
    """Checks that all labels in `filename` are prefixed according to the function defined in the file"""
    status_update("asm-labels", filename)

    with open(filename, "r") as f:
        content = f.read()

    # Find function symbol name
    func_pattern = r"MLK_ASM_FN_SYMBOL\((.*)\)"
    res = re.search(func_pattern, content)
    if res is None:
        raise Exception(f"Could not find function symbol in assembly file {filename}")
    funcname = res.group(1)
    lbl_prefix = funcname.replace("_asm", "") + "_"
    lbl_pattern = r"^(\w+):"
    for m in re.finditer(lbl_pattern, content, flags=re.M):
        lbl = m.group(1)
        if not lbl.startswith(lbl_prefix):
            raise Exception(
                f"Please change label {lbl} in {filename} to be prefixed by {lbl_prefix}"
            )


def check_asm_loop_labels():
    # Operate on assembly files in dev/ only. The ones in mlkem/ are autogenerated
    # from that and don't have the original MLK_ASM_FN_SYMBOL marker anymore.
    files = list(filter(lambda s: s.startswith("dev/"), get_asm_source_files()))
    with ThreadPoolExecutor() as executor:
        results = list(executor.map(check_asm_loop_labels_for_file, files))


def update_via_simpasm(
    infile_full,
    outdir,
    outfile=None,
    cflags=None,
    preserve_header=True,
    dry_run=False,
    force_cross=False,
):
    status_update("simpasm", infile_full)

    _, infile = os.path.split(infile_full)
    if outfile is None:
        outfile = infile
    outfile_full = os.path.join(outdir, outfile)

    # Check if we need to use a cross-compiler
    if "aarch64" in infile_full:
        source_arch = "aarch64"
    elif "x86_64" in infile_full:
        source_arch = "x86_64"
    else:
        raise Exception(f"Could not detect architecture of source file {infile_full}.")
    # Check native architecture
    if platform.machine().lower() in ["arm64", "aarch64"]:
        native_arch = "aarch64"
    else:
        native_arch = "x86_64"

    if native_arch != source_arch:
        cross_prefix = f"{source_arch}-unknown-linux-gnu-"
        cross_gcc = cross_prefix + "gcc"
        # Check if cross-compiler is present
        if shutil.which(cross_gcc) is None:
            if force_cross is False:
                return
            raise Exception(f"Could not find cross toolchain {cross_prefix}")
    else:
        cross_prefix = None

    with tempfile.NamedTemporaryFile(suffix=".S") as tmp:
        try:
            # Determine architecture from filename
            arch = "aarch64" if "aarch64" in infile_full else "x86_64"

            cmd = [
                "./scripts/simpasm",
                "--objdump=llvm-objdump",
                "--cfify",
                "--arch=" + arch,
                "-i",
                infile_full,
                "-o",
                tmp.name,
            ]
            if cross_prefix is not None:
                # Stick with llvm-objdump for disassembly
                cmd += ["--cc", cross_prefix + "gcc"]
                cmd += ["--nm", cross_prefix + "nm"]
            if cflags is not None:
                cmd += [f'--cflags="{cflags}"']
            if preserve_header is True:
                cmd += ["-p"]
            r = subprocess.run(
                cmd,
                stdout=subprocess.DEVNULL,
                stderr=subprocess.PIPE,
                check=True,
                text=True,
            )
        except subprocess.CalledProcessError as e:
            print(f"Command failed: {' '.join(cmd)}")
            print(f"Exit code: {e.returncode}")
            print(f"stderr: {e.stderr}")
            raise Exception("Failed to run simpasm") from e
        tmp.seek(0)
        new_contents = tmp.read().decode()

    update_file(outfile_full, new_contents, dry_run=dry_run)


def gen_hol_light_asm_file(job, dry_run=False):
    infile, outfile, indir, cflags = job
    update_via_simpasm(
        f"{indir}/{infile}",
        "proofs/hol_light/arm/mlkem",
        outfile=outfile,
        cflags=cflags,
        preserve_header=False,
        dry_run=dry_run,
    )


def gen_hol_light_asm(dry_run=False):

    joblist = [
        (
            "ntt.S",
            "mlkem_ntt.S",
            "dev/aarch64_opt/src",
            "-march=armv8.4-a+sha3 -Imlkem/src/native/aarch64/src",
        ),
        (
            "intt.S",
            "mlkem_intt.S",
            "dev/aarch64_opt/src",
            "-march=armv8.4-a+sha3 -Imlkem/src/native/aarch64/src",
        ),
        (
            "poly_tomont_asm.S",
            "mlkem_poly_tomont.S",
            "dev/aarch64_opt/src",
            "-march=armv8.4-a+sha3 -Imlkem/src/native/aarch64/src",
        ),
        (
            "poly_tobytes_asm.S",
            "mlkem_poly_tobytes.S",
            "dev/aarch64_opt/src",
            "-march=armv8.4-a+sha3 -Imlkem/src/native/aarch64/src",
        ),
        (
            "poly_reduce_asm.S",
            "mlkem_poly_reduce.S",
            "dev/aarch64_opt/src",
            "-march=armv8.4-a+sha3 -Imlkem/src/native/aarch64/src",
        ),
        (
            "poly_mulcache_compute_asm.S",
            "mlkem_poly_mulcache_compute.S",
            "dev/aarch64_opt/src",
            "-march=armv8.4-a+sha3 -Imlkem/src/native/aarch64/src",
        ),
        (
            "polyvec_basemul_acc_montgomery_cached_asm_k2.S",
            "mlkem_poly_basemul_acc_montgomery_cached_k2.S",
            "dev/aarch64_opt/src",
            "-march=armv8.4-a+sha3 -Imlkem/src/native/aarch64/src",
        ),
        (
            "polyvec_basemul_acc_montgomery_cached_asm_k3.S",
            "mlkem_poly_basemul_acc_montgomery_cached_k3.S",
            "dev/aarch64_opt/src",
            "-march=armv8.4-a+sha3 -Imlkem/src/native/aarch64/src",
        ),
        (
            "polyvec_basemul_acc_montgomery_cached_asm_k4.S",
            "mlkem_poly_basemul_acc_montgomery_cached_k4.S",
            "dev/aarch64_opt/src",
            "-march=armv8.4-a+sha3 -Imlkem/src/native/aarch64/src",
        ),
        (
            "rej_uniform_asm.S",
            "mlkem_rej_uniform.S",
            "dev/aarch64_opt/src",
            "-march=armv8.4-a+sha3 -Imlkem/src/native/aarch64/src",
        ),
        (
            "keccak_f1600_x1_scalar_asm.S",
            "keccak_f1600_x1_scalar.S",
            "dev/fips202/aarch64/src",
            "-march=armv8.4-a+sha3 -Imlkem/src/fips202/native/aarch64/src/",
        ),
        (
            "keccak_f1600_x1_v84a_asm.S",
            "keccak_f1600_x1_v84a.S",
            "dev/fips202/aarch64/src",
            "-march=armv8.4-a+sha3 -Imlkem/src/fips202/native/aarch64/src/",
        ),
        (
            "keccak_f1600_x2_v84a_asm.S",
            "keccak_f1600_x2_v84a.S",
            "dev/fips202/aarch64/src",
            "-march=armv8.4-a+sha3 -Imlkem/src/fips202/native/aarch64/src/",
        ),
        (
            "keccak_f1600_x4_v8a_v84a_scalar_hybrid_asm.S",
            "keccak_f1600_x4_v8a_v84a_scalar.S",
            "dev/fips202/aarch64/src",
            "-march=armv8.4-a+sha3 -Imlkem/src/fips202/native/aarch64/src/",
        ),
        (
            "keccak_f1600_x4_v8a_scalar_hybrid_asm.S",
            "keccak_f1600_x4_v8a_scalar.S",
            "dev/fips202/aarch64/src",
            "-march=armv8.4-a+sha3 -Imlkem/src/fips202/native/aarch64/src/",
        ),
    ]

    with ThreadPoolExecutor() as executor:
        _ = list(
            executor.map(partial(gen_hol_light_asm_file, dry_run=dry_run), joblist)
        )


def update_via_copy(infile_full, outfile_full, dry_run=False, transform=None):
    status_update("copy", f"{infile_full} -> {outfile_full}")

    with open(infile_full, "r") as f:
        content = f.read()

    if transform is not None:
        content = transform(content)

    update_file(outfile_full, content, dry_run=dry_run)


def update_via_remove(filename, dry_run=False):
    if dry_run is True:
        print(
            f"Autogenerated file {filename} needs removing. Have you called scripts/autogen?",
            file=sys.stderr,
        )
        exit(1)

    # Remove the file
    os.remove(filename)


def synchronize_file(
    f, in_dir, out_dir, dry_run=False, delete=False, no_simplify=False, **kwargs
):

    # Only synchronize sources, but not README.md, Makefile and so on
    extensions = (".c", ".h", ".i", ".inc", ".S")

    if not f.endswith(extensions):
        return None

    basename = os.path.basename(f)

    if delete is True:
        return basename

    if no_simplify is False and f.endswith(".S"):
        update_via_simpasm(f, out_dir, dry_run=dry_run, **kwargs)
    else:
        # Update via copy
        _, infile = os.path.split(f)
        outfile_full = os.path.join(out_dir, infile)
        # The header guards will also be checked later, but if we
        # don't do it here, the dry-run would fail because of a
        # mismatching intermediate file
        if f.endswith(".h"):
            transform = lambda c: adjust_header_guard_for_filename(c, outfile_full)
        else:
            transform = None
        update_via_copy(f, outfile_full, dry_run=dry_run, transform=transform)

    return basename


def synchronize_backend(
    in_dir, out_dir, dry_run=False, delete=False, no_simplify=False, **kwargs
):
    copied = []

    with ThreadPoolExecutor() as executor:
        pool_results = list(
            executor.map(
                partial(
                    synchronize_file,
                    in_dir=in_dir,
                    out_dir=out_dir,
                    dry_run=dry_run,
                    delete=delete,
                    no_simplify=no_simplify,
                    **kwargs,
                ),
                get_files(os.path.join(in_dir, "*")),
            )
        )

    copied = [r for r in pool_results if r is not None]

    if delete is False:
        return

    # Check for files in the target directory that have not been copied
    for f in get_files(os.path.join(out_dir, "*")):
        if os.path.basename(f) in copied:
            continue
        # Otherwise, remove it
        update_via_remove(f, dry_run=dry_run)


def synchronize_backends(
    *, dry_run=False, force_cross=False, clean=False, delete=False, no_simplify=False
):
    if clean is False:
        ty = "opt"
    else:
        ty = "clean"

    if delete is False:
        # We may switch the AArch64 arithmetic backend, so adjust the metadata file
        update_via_copy(
            f"dev/aarch64_{ty}/meta.h",
            "mlkem/src/native/aarch64/meta.h",
            dry_run=dry_run,
            transform=lambda c: adjust_header_guard_for_filename(
                c, "mlkem/src/native/aarch64/meta.h"
            ),
        )

        update_via_copy(
            f"dev/x86_64/meta.h",
            "mlkem/src/native/x86_64/meta.h",
            dry_run=dry_run,
            transform=lambda c: adjust_header_guard_for_filename(
                c, "mlkem/src/native/x86_64/meta.h"
            ),
        )

    synchronize_backend(
        f"dev/aarch64_{ty}/src",
        "mlkem/src/native/aarch64/src",
        dry_run=dry_run,
        delete=delete,
        force_cross=force_cross,
        no_simplify=no_simplify,
        cflags="-Imlkem/src/native/aarch64/src",
    )
    synchronize_backend(
        "dev/fips202/aarch64/src",
        "mlkem/src/fips202/native/aarch64/src",
        dry_run=dry_run,
        delete=delete,
        force_cross=force_cross,
        no_simplify=no_simplify,
        cflags="-Imlkem/src/fips202/native/aarch64/src -march=armv8.4-a+sha3",
    )
    synchronize_backend(
        "dev/fips202/aarch64",
        "mlkem/src/fips202/native/aarch64",
        dry_run=dry_run,
        delete=delete,
        force_cross=force_cross,
        no_simplify=no_simplify,
        cflags="-Imlkem/src/fips202/native/aarch64 -march=armv8.4-a+sha3",
    )
    synchronize_backend(
        "dev/x86_64/src",
        "mlkem/src/native/x86_64/src",
        dry_run=dry_run,
        delete=delete,
        force_cross=force_cross,
        no_simplify=no_simplify,
        # Turn off control-flow protection (CET) explicitly. Newer versions of
        # clang turn it on by default and insert endbr64 instructions at every
        # global symbol.
        # We insert endbr64 instruction manually via the MLK_ASM_FN_SYMBOL
        # macro.
        # This leads to duplicate endbr64 instructions causing a failure when
        # comparing the object code before and after simplification.
        cflags="-Imlkem/src/native/x86_64/src/ -mavx2 -mbmi2 -msse4 -fcf-protection=none",
    )


def adjust_header_guard_for_filename(content, header_file):

    status_update("header guards", header_file)

    content = content.split("\n")
    exceptions = {"mlkem/mlkem_native.h": "MLK_H"}

    # Use full filename as the header guard, with '/' and '.' replaced by '_'
    guard_name = (
        header_file.removeprefix("mlkem/src/")
        .replace("/", "_")
        .replace(".", "_")
        .upper()
    )
    guard_name = "MLK_" + guard_name

    if header_file in exceptions.keys():
        guard_name = exceptions[header_file]

    def gen_guard():
        yield f"#ifndef {guard_name}"
        yield f"#define {guard_name}"

    def gen_footer():
        yield f"#endif"
        yield ""

    guard = list(gen_guard())
    footer = list(gen_footer())

    # Skip over initial commentary
    insert_at = None
    for i, l in enumerate(content):
        if l.strip() == "" or l.startswith(("/*", " *")):
            continue
        insert_at = i
        break

    i = insert_at
    while content[i].strip() == "":
        i += 1
    # Check if header file has some guard -- if so, drop it
    if content[i].strip().startswith("#if !defined") or content[i].strip().startswith(
        "#ifndef"
    ):
        del content[i]
        if content[i].strip().startswith("#define"):
            del content[i]
        has_guard = True
    else:
        has_guard = False
    # Add standardized guard
    content = content[:i] + guard + content[i:]
    # Check if header has some footer
    if (
        has_guard is True
        and content[-1] == ""
        and content[-2].strip().startswith("#endif")
    ):
        del content[-2:]
    # Add standardized footer
    content = content + footer

    return "\n".join(content)


def gen_header_guard(header_file, dry_run=False):
    with open(header_file, "r") as f:
        content = f.read()
    new_content = adjust_header_guard_for_filename(content, header_file)
    update_file(header_file, new_content, dry_run=dry_run)


def gen_header_guards(dry_run=False):
    with ThreadPoolExecutor() as executor:
        _ = list(
            executor.map(
                partial(gen_header_guard, dry_run=dry_run),
                get_header_files(core_only=True),
            )
        )


def gen_source_undefs(source_file, dry_run=False):
    status_update("undefs", source_file)

    # Get list of #define's clauses in this source file (ignore filename)
    undef_list = list(map(lambda c: c[1], get_defines_from_file(source_file)))
    # Get define clauses from header files, as dict
    header_defs = {d: c for (c, d) in get_defines()}

    undefs = []
    ignored = []
    for d in undef_list:
        if d not in header_defs.keys():
            undefs.append(f"#undef {d}")
        else:
            ignored.append((d, header_defs[d]))
    info_line = "/* Some macros are kept because they are also defined in a header. */"
    if len(ignored) != 0:
        undefs.append(
            "/* Some macros are kept because they are also defined in a header. */"
        )
        for d, c in ignored:
            undefs.append(f"/* Keep: {d} ({c.split('/')[-1]}) */")

    # Remove list of #undef's at the end of the source file, and add
    # the up-to-date one.
    with open(source_file, "r") as f:
        content = f.read().split("\n")

    footer_start = None
    if source_file.endswith(".S"):
        # Try to split off simpasm footer
        footer_start_marker = "simpasm: footer-start"
        for i, l in enumerate(content):
            if footer_start_marker in l:
                footer_start = i
                break
    if footer_start is not None:
        simpasm_footer = content[footer_start:]
        content = content[:footer_start]
    else:
        simpasm_footer = []

    while True:
        l = content[-1].strip()
        if (
            l.startswith("#undef")
            or l == info_line
            or l.startswith("/* Keep:")
            or l == ""
        ):
            del content[-1]
            continue
        break

    footer = [
        "",
        "/* To facilitate single-compilation-unit (SCU) builds, undefine all macros.",
        " * Don't modify by hand -- this is auto-generated by scripts/autogen. */",
    ]
    # Remove existing footer, if present
    if content[-len(footer) :] == footer:
        content = content[: -len(footer)]

    if len(undefs) != 0:
        content = content + footer + undefs + [""]
    else:
        content = content + [""]

    new_content = "\n".join(content + simpasm_footer)
    update_file(source_file, new_content, dry_run=dry_run)


def gen_undefs(dry_run=False):
    with ThreadPoolExecutor() as executor:
        _ = list(
            executor.map(
                partial(gen_source_undefs, dry_run=dry_run),
                get_c_source_files(core_only=True)
                + get_asm_source_files(core_only=True),
            )
        )


def gen_slothy(funcs, dry_run=False):
    if dry_run is True:
        # Do nothing for a dry run
        return

    if not isinstance(funcs, list):
        return

    targets = list(map(lambda s: s + ".S", funcs))

    for t in targets:
        status_update("SLOTHY", f"Regenerating {t}")

        if t.startswith("keccak"):
            base = "dev/fips202/aarch64/src"
        else:
            base = "dev/aarch64_opt/src"

        # Remove file(s) to be re-generated
        if t.endswith(".S"):
            subprocess.run(["rm", "-f", f"{base}/{t}"])

        p = subprocess.run(["make", t] + ["-C", base])
        if p.returncode != 0:
            print(f"Failed to run SLOTHY on {t}!")
            exit(1)


class BibliographyEntry:
    def __init__(self, raw_dict):
        self._raw = raw_dict
        self._usages = []

    def register_usages(self, lst):
        self._usages += lst

    @property
    def usages(self):
        return self._usages

    @property
    def name(self):
        return self._raw["name"]

    @property
    def short(self):
        if "short" in self._raw.keys():
            return self._raw["short"]
        return self.name

    @property
    def id(self):
        return self._raw["id"]

    @property
    def url(self):
        return self._raw["url"]

    @staticmethod
    def full_name(name):
        if "," not in name:
            return name
        surname, forename = name.split(",")
        return forename.strip() + " " + surname.strip()

    @property
    def authors(self):
        authors = self._raw["author"]
        if not isinstance(authors, list):
            authors = [authors]
        authors = list(map(BibliographyEntry.full_name, authors))
        return authors

    @property
    def authors_text(self):
        authors = self._raw["author"]
        if not isinstance(authors, list):
            authors = [authors]

        def surname(name):
            return name.split(",")[0].strip()

        if len(authors) > 1:
            authors = ", ".join(map(surname, authors))
        else:
            authors = BibliographyEntry.full_name(authors[0])
        return authors


def gen_markdown_citations_for(filename, bibliography, dry_run=False):

    # Skip BIBLIOGRAPHY.md
    if filename == "BIBLIOGRAPHY.md":
        return

    with open(filename, "r") as f:
        content = f.read()
    content = content.split("\n")

    # Lookup all citations in style `[^ID]`
    citations = {}
    for i, l in enumerate(content):
        for m in re.finditer(r"\[\^(?P<id>\w+)\]", l):
            cite_id = m.group("id")
            uses = citations.get(cite_id, [])
            uses.append((filename, i))
            citations[cite_id] = uses

    # Find and remove any existing citation footnotes
    footnote_footer_start = "<!--- bibliography --->"
    try:
        i = content.index(footnote_footer_start)
        content = content[:i]
    except ValueError:
        pass

    # Add footnotes for all citations found
    if len(citations) > 0:
        content.append(footnote_footer_start)
    cite_ids = list(citations.keys())
    cite_ids.sort()
    for cite_id in cite_ids:
        uses = citations[cite_id]
        entry = bibliography.get(cite_id, None)
        if entry is None:
            raise Exception(
                f"Could not find bibliography entry {cite_id} referenced in {filename}. Known entries: {list(bibliography.keys())}"
            )
        content.append(
            f"[^{cite_id}]: {entry.authors_text}: {entry.name}, [{entry.url}]({entry.url})"
        )

        # Remember this usage of the bibliography entry
        entry.register_usages(uses)

    if len(citations) > 0:
        content.append("")

    update_file(filename, "\n".join(content), dry_run=dry_run)


def gen_c_citations_for(filename, bibliography, dry_run=False):

    with open(filename, "r") as f:
        content = f.read()

    references_start = [
        "/* References",
        " * ==========",
    ]
    references_end = [" */"]

    # Find and remove any existing reference section
    ref_pattern = r"/\* (# )?References.*?\*/\n+"
    content = re.sub(ref_pattern, "", content, flags=re.DOTALL)

    content = content.split("\n")

    # Lookup all citations in style `@[ID]`
    citations = {}
    for i, l in enumerate(content):
        for m in re.finditer(r"@\[(?P<id>\w+)", l):
            cite_id = m.group("id")
            uses = citations.get(cite_id, [])
            # Remember usage. +1 because line counting starts at 1
            uses.append((filename, i + 1))
            citations[cite_id] = uses

    # Add references section
    references = []
    references += references_start

    cite_ids = list(citations.keys())
    cite_ids.sort()
    for cite_id in cite_ids:
        uses = citations[cite_id]
        entry = bibliography.get(cite_id, None)
        if entry is None:
            raise Exception(
                f"Could not find bibliography entry {cite_id} referenced in {filename}"
            )
        references.append(f" *")
        references.append(f" * - [{cite_id}]")
        references.append(f" *   {entry.name}")
        references.append(f" *   {entry.authors_text}")
        references.append(f" *   {entry.url}")

    references += references_end

    # Pre-update formatting messes up overly long comment lines which are
    # likely here. Thus, we explicitly format and fix bad indentation manually.
    references = "\n".join(references)
    references = format_content(references)
    references = re.sub(r" \* (\w.*)", r" *   \1", references)
    references = references.split("\n")
    references = [""] + references

    if len(cite_ids) > 0:
        # Add references to file after initial header section
        # Skip over copyright
        insert_at = None
        for i, l in enumerate(content):
            if l.startswith(("/*", " *")):
                continue
            insert_at = i
            break
        content = content[:insert_at] + references + content[insert_at:]

    # Remember uses -- needs to happen after insertion of references
    # since we need to adjust the line count
    for cite_id in cite_ids:
        uses = citations[cite_id]
        entry = bibliography.get(cite_id, None)

        # Adjust line count after insertion of references
        def bump_line_count(x):
            return (x[0], x[1] + len(references))

        uses = list(map(bump_line_count, uses))

        # Remember this usage of the bibliography entry
        entry.register_usages(uses)

    update_file(
        filename, "\n".join(content), dry_run=dry_run, skip_preprocessor_comments=True
    )


def gen_citations_for(filename, bibliography, dry_run=False):
    status_update("citations", filename)
    if filename.endswith(".md"):
        gen_markdown_citations_for(filename, bibliography, dry_run=dry_run)
    elif filename.endswith((".c", ".h", ".S")):
        gen_c_citations_for(filename, bibliography, dry_run=dry_run)
    else:
        raise Exception(f"Unexpected file extension in {filename}")


def gen_bib_file(bibliography, dry_run=False):

    content = [
        "[//]: # (SPDX-License-Identifier: CC-BY-4.0)",
        "[//]: # (This file is auto-generated from BIBLIOGRAPHY.yml)",
        "[//]: # (Do not modify it directly)",
        "",
        "# Bibliography",
        "",
        "This file lists the citations made throughout the mlkem-native ",
        "source code and documentation.",
        "",
    ]

    cite_ids = list(bibliography.keys())
    cite_ids.sort()

    for cite_id in cite_ids:
        entry = bibliography[cite_id]
        content.append(f"### `{cite_id}`")
        content.append("")
        content.append(f"* {entry.name}")
        content.append(f"* Author(s):")
        for author in entry.authors:
            content.append(f"  - {author}")
        content.append(f"* URL: {entry.url}")
        content.append(f"* Referenced from:")
        # Usages are pairs of (filename, line_count)
        # Ignore line_count for now, as it would require `autogen` after
        # a change to source files.
        files = list(set(map(lambda x: x[0], entry.usages)))
        files.sort()
        for filename in files:
            content.append(f"  - [{filename}]({filename})")
        content.append("")

    update_file("BIBLIOGRAPHY.md", "\n".join(content), dry_run=dry_run)


def get_oqs_shared_sources(backend):
    """Get shared source files for OQS integration"""
    mlkem_dir = "mlkem/src"

    # add files mlkem/src/*
    sources = [
        f"mlkem/src/{f}"
        for f in os.listdir(mlkem_dir)
        if os.path.isfile(f"{mlkem_dir}/{f}") and not f.endswith(".o")
    ]
    # add files mlkem/src/native/* (API definitions)
    sources += [
        f"mlkem/src/native/{f}"
        for f in os.listdir(f"{mlkem_dir}/native")
        if os.path.isfile(f"{mlkem_dir}/native/{f}")
    ]
    # We use a custom config
    sources.remove("mlkem/src/config.h")
    # Add FIPS202 glue code
    sources += [
        "integration/liboqs/fips202_glue.h",
        "integration/liboqs/fips202x4_glue.h",
    ]
    # Add custom config
    if backend == "ref":
        backend = "c"
    sources.append(f"integration/liboqs/config_{backend.lower()}.h")

    return sources


def get_oqs_native_sources(backend):
    """Get native source files for OQS integration"""
    return [f"mlkem/src/native/{backend}"]


def gen_oqs_meta_file(filename, dry_run=False):
    """Generate OQS META.yml file with updated source lists"""
    status_update("OQS META", filename)

    with open(filename, "r") as f:
        content = f.read()

    # Parse YAML while preserving structure
    yml_data = yaml.safe_load(content)

    for impl in yml_data["implementations"]:
        name = impl["name"]

        sources = get_oqs_shared_sources(name)

        # NOTE: Sorting at the end causes the libOQS importer to fail.
        # Somehow, the native directory cannot be imported too early.
        sources.sort()

        if name != "ref":
            sources += get_oqs_native_sources(name)
        impl["sources"] = " ".join(sources)

    # Convert back to YAML string with standard copyright header
    yaml_header = "\n".join(gen_yaml_header())

    new_content = yaml.dump(
        yml_data,
        default_flow_style=False,
        sort_keys=False,
        allow_unicode=True,
        encoding=None,
    )

    # Combine copyright header with new YAML content
    new_content = yaml_header + new_content

    update_file(filename, new_content, dry_run=dry_run)


def gen_oqs_meta_files(dry_run=False):
    """Generate all OQS META.yml files"""
    meta_files = [
        "integration/liboqs/ML-KEM-512_META.yml",
        "integration/liboqs/ML-KEM-768_META.yml",
        "integration/liboqs/ML-KEM-1024_META.yml",
    ]

    for meta_file in meta_files:
        gen_oqs_meta_file(meta_file, dry_run=dry_run)


def gen_citations(dry_run=False):
    # Load bibliography
    with open("BIBLIOGRAPHY.yml", "r") as f:
        bibliography_raw = yaml.safe_load(f.read())

    bibliography = {}
    for r in bibliography_raw:
        cite_id = r["id"]
        bibliography[cite_id] = BibliographyEntry(r)

    with ThreadPoolExecutor() as executor:
        _ = list(
            executor.map(
                partial(
                    gen_citations_for,
                    bibliography=bibliography,
                    dry_run=dry_run,
                ),
                get_markdown_files()
                + get_asm_source_files()
                + get_c_source_files()
                + get_header_files(),
            )
        )

    # Check that every bibliography entry has been used as least once
    for e in bibliography.values():
        if len(e.usages) == 0:
            raise Exception(
                f"Bibliography entry {e.id} is unused! "
                "Add a citation or remove from BIBLIOGRAPHY.yml."
            )

    gen_bib_file(bibliography, dry_run=False)


def _main():
    slothy_choices = [
        "ntt",
        "intt",
        "poly_tobytes_asm",
        "poly_tomont_asm",
        "poly_reduce_asm",
        "poly_mulcache_compute_asm",
        "polyvec_basemul_acc_montgomery_cached_asm_k2",
        "polyvec_basemul_acc_montgomery_cached_asm_k3",
        "polyvec_basemul_acc_montgomery_cached_asm_k4",
        "rej_uniform_asm",
        "keccak_f1600_x1_scalar_asm",
        "keccak_f1600_x4_v8a_scalar_hybrid_asm",
        "keccak_f1600_x4_v8a_v84a_scalar_hybrid_asm",
    ]

    parser = argparse.ArgumentParser(
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument("--dry-run", default=False, action="store_true")
    parser.add_argument("--slothy", nargs="*", default=None, choices=slothy_choices)
    parser.add_argument("--aarch64-clean", default=False, action="store_true")
    parser.add_argument("--no-simplify", default=False, action="store_true")
    parser.add_argument("--force-cross", default=False, action="store_true")

    args = parser.parse_args()

    os.chdir(os.path.join(os.path.dirname(__file__), ".."))

    gen_citations(args.dry_run)
    high_level_status("Generated citations")

    gen_oqs_meta_files(args.dry_run)
    high_level_status("Generated OQS META.yml files")

    if args.slothy == []:
        args.slothy = slothy_choices
    if args.slothy is not None:
        gen_slothy(args.slothy, args.dry_run)
        high_level_status("Generated SLOTHY optimized assembly")

    check_asm_register_aliases()
    high_level_status("Checked assembly register aliases")
    check_asm_loop_labels()
    high_level_status("Checked assembly loop labels")

    gen_c_zeta_file(args.dry_run)
    gen_aarch64_hol_light_zeta_file(args.dry_run)
    gen_aarch64_hol_light_rej_uniform_table(args.dry_run)
    gen_aarch64_zeta_file(args.dry_run)
    gen_aarch64_rej_uniform_table(args.dry_run)
    gen_avx2_zeta_file(args.dry_run)
    gen_avx2_rej_uniform_table(args.dry_run)
    gen_avx2_mulcache_twiddles_file(args.dry_run)
    high_level_status("Generated zeta and lookup tables")

    if platform.machine().lower() in ["arm64", "aarch64"]:
        gen_hol_light_asm(args.dry_run)
        high_level_status("Generated HOL Light assembly")

    synchronize_backends(
        dry_run=args.dry_run,
        clean=args.aarch64_clean,
        no_simplify=args.no_simplify,
        force_cross=args.force_cross,
    )
    high_level_status("Synchronized backends")

    gen_header_guards(args.dry_run)
    high_level_status("Generated header guards")
    gen_preprocessor_comments(args.dry_run)
    high_level_status("Generated preprocessor comments")
    gen_monolithic_source_file(args.dry_run)
    gen_monolithic_asm_file(args.dry_run)
    high_level_status("Generated monolithic source files")
    gen_undefs(args.dry_run)
    high_level_status("Generated undefs")

    synchronize_backends(
        dry_run=args.dry_run,
        clean=args.aarch64_clean,
        delete=True,
        force_cross=args.force_cross,
        no_simplify=args.no_simplify,
    )
    high_level_status("Completed final backend synchronization")

    check_macro_typos()
    high_level_status("Checked macro typos")


if __name__ == "__main__":
    _main()
