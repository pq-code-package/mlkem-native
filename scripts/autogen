#!/usr/bin/env python3
# Copyright (c) The mlkem-native project authors
# SPDX-License-Identifier: Apache-2.0 OR ISC OR MIT

import subprocess
import tempfile
import platform
import argparse
import shutil
import pathlib
import re
import sys
import pyparsing as pp
import os
import yaml

from concurrent.futures import ThreadPoolExecutor
from functools import partial

modulus = 3329
root_of_unity = 17
montgomery_factor = pow(2, 16, modulus)

# This file re-generated auto-generated source files in mlkem-native.
#
# It currently covers:
# - zeta values for the reference NTT and invNTT
# - lookup tables used for fast rejection sampling
# - source files for monolithic single-CU build
# - simplified assembly sources
# - header guards
# - #undef's for CU-local macros


def status_update(task, msg):
    print(f"\r{'':<140}", end="", flush=True)
    print(f"\r[{task}]: {msg} ...", end="", flush=True)


def gen_header():
    yield "/*"
    yield " * Copyright (c) The mlkem-native project authors"
    yield " * SPDX-License-Identifier: Apache-2.0 OR ISC OR MIT"
    yield " */"
    yield ""
    yield "/*"
    yield " * WARNING: This file is auto-generated from scripts/autogen"
    yield " *          Do not modify it directly."
    yield " */"
    yield ""


def gen_hol_light_header():
    yield "(*"
    yield " * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved."
    yield " * SPDX-License-Identifier: Apache-2.0 OR ISC OR MIT-0"
    yield " *)"
    yield ""
    yield "(*"
    yield " * WARNING: This file is auto-generated from scripts/autogen"
    yield " *          Do not modify it directly."
    yield " *)"
    yield ""


def format_content(content):
    p = subprocess.run(
        ["clang-format"], capture_output=True, input=content, text=True, shell=True
    )
    if p.returncode != 0:
        print(p.stderr)
        print(
            f"Failed to auto-format autogenerated code (clang-format return code {p.returncode}). Are you running in a nix shell? See BUILDING.md."
        )
        exit(1)
    return p.stdout


class CondParser:
    """Rudimentary parser for expressions if `#if .. #else ..` directives"""

    def __init__(self):
        c_identifier = pp.common.identifier()
        c_integer_suffix = pp.one_of("U L LU UL LL ULL LLU", caseless=True)
        c_dec_integer = pp.Combine(
            pp.Optional(pp.one_of("+ -"))
            + pp.Word(pp.nums)
            + pp.Optional(c_integer_suffix)
        )
        c_hex_integer = pp.Combine(
            pp.Literal("0x") + pp.Word(pp.hexnums) + pp.Optional(c_integer_suffix)
        )

        self.parser = pp.infix_notation(
            c_identifier | c_hex_integer | c_dec_integer,
            [
                (pp.one_of("!"), 1, pp.opAssoc.RIGHT),
                (pp.one_of("!= == <= >= > <"), 2, pp.opAssoc.LEFT),
                (pp.one_of("&&"), 2, pp.opAssoc.LEFT),
                (pp.one_of("||"), 2, pp.opAssoc.LEFT),
            ],
        )

    @staticmethod
    def connective(res):
        """Extract the top-level connective for the expression"""
        if not isinstance(res, list):
            return None
        elif len(res) == 2:
            # Unary operator (will be "!" in our case)
            return res[0]
        else:
            # Binary operator
            return res[1]

    @staticmethod
    def map_top(f, res):
        """Apply function to arguments of top-level connective"""
        if not isinstance(res, list):
            return res
        else:
            # We expect `f` to do nothing on strings, so it is safe
            # to apply it everywhere, including the connectives.
            return list(map(f, res))

    @staticmethod
    def args(res):
        """Assuming the argument is a binary operation, return all arguments"""
        return res[::2]

    @staticmethod
    def simplify_double_negation(res):
        """Cancel double negations"""
        if CondParser.connective(res) == "!" and CondParser.connective(res[1]) == "!":
            res = res[1][1]
        res = CondParser.map_top(CondParser.simplify_double_negation, res)
        return res

    @staticmethod
    def simplify_not_eq(res):
        """Replace !(x == y) by x != y, and !(x != y) by x == y"""
        if CondParser.connective(res) == "!" and CondParser.connective(res[1]) == "==":
            res = res[1]
            res[1] = "!="
        if CondParser.connective(res) == "!" and CondParser.connective(res[1]) == "!=":
            res = res[1]
            res[1] = "=="
        res = CondParser.map_top(CondParser.simplify_not_eq, res)
        return res

    @staticmethod
    def simplify_neq_chain(res):
        """Check for &&-chains of inequalities followed by an equality
        which implies the inequality. This catches patterns like
        ```
         #if MLKEM_K == 2
         ...
         #elif MLKEM_K == 3
         ...
         #elif MLKEM_K == 4
         ...
         #endif
        ```
        """
        if (
            CondParser.connective(res) == "&&"
            and CondParser.connective(res[-1]) == "=="
        ):
            lhs = res[-1][0]
            rhs = res[-1][2]
            args = []
            for a in CondParser.args(res[:-1]):
                if CondParser.connective(a) == "!=" and a[0] == lhs:
                    args.append(a[2])
                else:
                    args = None
                    break
            if args is None:
                return
            # Check if all args are numerical and different
            if rhs.isdigit() and all(
                map(lambda a: a.isdigit() and int(a) != int(rhs), args)
            ):
                # Success -- just drop all but the final condition
                return res[-1]
        res = CondParser.map_top(CondParser.simplify_neq_chain, res)
        return res

    @staticmethod
    def print_exp(exp, inner=False):
        conn = CondParser.connective(exp)
        if conn is None:
            return exp
        elif conn == "!":
            res = f"!{CondParser.print_exp(exp[1], inner=True)}"
        else:
            padded_conn = f" {conn} "
            res = padded_conn.join(
                map(lambda e: CondParser.print_exp(e, inner=True), CondParser.args(exp))
            )
        if inner is True and conn in ["&&", "||"]:
            res = f"({res})"
        return res

    def simplify_assoc(exp):
        """Check for unnecesary bracketing and remove it"""
        conn = CondParser.connective(exp)
        if conn in ["&&", "||"]:
            args = CondParser.args(exp)
            new_args = []
            for a in args:
                if CondParser.connective(a) == conn:
                    new_args += CondParser.args(a)
                else:
                    new_args.append(a)
            exp = [x for y in map(lambda x: [x, conn], new_args) for x in y][:-1]
        exp = CondParser.map_top(CondParser.simplify_assoc, exp)
        return exp

    def simplify_all(exp):
        exp = CondParser.simplify_double_negation(exp)
        exp = CondParser.simplify_not_eq(exp)
        exp = CondParser.simplify_neq_chain(exp)
        exp = CondParser.simplify_assoc(exp)
        return exp

    def parse_condition(self, exp, simplify=True):
        try:
            exp = self.parser.parseString(exp, parseAll=True).as_list()[0]
        except pp.ParseException:
            print(f"WARNING: Ignoring condition '{exp}' I cannot parse")
            return exp
        if simplify is True:
            exp = CondParser.simplify_all(exp)
        return exp

    def normalize_condition(self, exp):
        return CondParser.print_exp(self.parse_condition(exp))


def adjust_preprocessor_comments_for_filename(content, source_file):
    """Automatically add comments to large `#if ... #else ... #endif`
    blocks indicating the guarding conditions.

    For example, a block

    ```c
      #if FOO
      ...
      #else
      ...
      #endif
    ```

    will be transformed into


    ```c
      #if FOO
      ...
      #else /* FOO */
      ...
      #endif /* !FOO */
    ```

    except when the distance between the preprocessor directives is
    very short, and the annotations would be more harmful than useful.

    ```
    """
    status_update("if-else", source_file)

    content = content.split("\n")
    new_content = []

    # Stack of `#if` statements. Every entry is a tuple
    # `(conds, line_no, if_or_else, has_children)`, where
    # - `conds` is the list of conditions being tested.
    #   In a normal `#if ... #else ...` braach, this is a singleton list
    #   containing the condition being tested. In a chain of
    #   `#if .. #elif ..` it contains all conditions encountered to this point.
    # - `line_no` is the line where it started
    # - `if_or_else` indicates whether we are in the `#if`
    #   or the `#else` branch (if present)
    # - `force_print` indicates if a comment should be omitted
    if_stack = []

    def merge_escaped_lines(l, i):
        while l.endswith("\\"):
            l = l.removesuffix("\\").rstrip() + content[i + 1].lstrip()
            i = i + 1
        return (l, i)

    def merge_commented_lines(l, i):
        # Not very robust, but good enough
        if not "/*" in l or "*/" in l:
            return (l, i)
        i += 1
        while "*/" not in content[i]:
            l += content[i]
            i += 1

        l += content[i]
        return (l, i)

    def should_print(cur_line_no, conds, line_no, force_print):
        line_threshold = 5
        if force_print is True:
            return True

        if cur_line_no - line_no >= line_threshold:
            return True
        return False

    parser = CondParser()

    def format_condition(cond):
        cond = re.sub(r"defined\(([^)]+)\)", r"\1", cond)
        return parser.normalize_condition(cond)

    def format_conditions(conds, branch):
        prev_conds = list(map(lambda s: f"!({s})", conds[:-1]))
        final_cond = conds[-1]
        if branch is False:
            final_cond = f"!({final_cond})"
        full_cond = "&&".join(prev_conds + [final_cond])
        return format_condition(full_cond)

    def adhoc_format(line):
        # .c and .h files are formatted as a whole
        if not source_file.endswith(".S"):
            return line
        return format_content(line)

    i = 0
    while i < len(content):
        l = content[i].strip()
        # Replace #ifdef by #if defined(...)
        if l.startswith("#ifdef "):
            l = "#if defined(" + l.removeprefix("#ifdef").strip() + ")"
        if l.startswith("#ifndef "):
            l = "#if !defined(" + l.removeprefix("#ifndef").strip() + ")"
        if l.startswith("#if"):
            l, _ = merge_escaped_lines(l, i)
            cond = l.removeprefix("#if")
            if_stack.append(([cond], i, True, False))
            new_content.append(content[i])
        elif l.startswith("#elif"):
            conds, _, _, force_print = if_stack.pop()
            l, _ = merge_escaped_lines(l, i)
            conds.append(l.removeprefix("#elif"))
            if_stack.append((conds, i, True, force_print))
            new_content.append(content[i])
        elif l.startswith("#else"):
            l, i = merge_escaped_lines(l, i)
            _, i = merge_commented_lines(l, i)
            conds, j, branch, force_print = if_stack.pop()
            assert branch is True
            print_else = should_print(i, cond, j, force_print)
            if_stack.append((conds, i, False, print_else))
            if print_else is True:
                cond = format_conditions(conds, True)
                new_content.append(adhoc_format("#else /* " + cond + " */"))
            else:
                new_content.append("#else")
        elif l.startswith("#endif"):
            l, i = merge_escaped_lines(l, i)
            _, i = merge_commented_lines(l, i)
            conds, j, branch, force_print = if_stack.pop()
            print_endif = should_print(i, conds, j, force_print)
            if print_endif is False:
                new_content.append("#endif")
            else:
                cond = format_conditions(conds, branch)
                new_content.append(adhoc_format("#endif /* " + cond + " */"))
        else:
            # Skip over multiline comments -- we don't want to
            # handle `#if ...` inside documentation as this would
            # lead to nested `/* ... */`.
            i_old = i
            _, i = merge_commented_lines(l, i_old)
            new_content += content[i_old : i + 1]
        i += 1

    return "\n".join(new_content)


def gen_preprocessor_comments_for(source_file, dry_run=False):
    with open(source_file, "r") as f:
        content = f.read()
    new_content = adjust_preprocessor_comments_for_filename(content, source_file)
    update_file(source_file, new_content, dry_run=dry_run)


def gen_preprocessor_comments(dry_run=False):
    files = get_c_source_files() + get_asm_source_files() + get_header_files()
    with ThreadPoolExecutor() as executor:
        results = list(
            executor.map(partial(gen_preprocessor_comments_for, dry_run=dry_run), files)
        )


def update_file(filename, content, dry_run=False, force_format=False):

    if force_format is True or filename.endswith((".c", ".h", ".i")):
        content = adjust_preprocessor_comments_for_filename(content, filename)
        content = format_content(content)

    if os.path.exists(filename) is True:
        with open(filename, "r") as f:
            current_content = f.read()
    else:
        current_content = None

    if current_content == content:
        return

    if dry_run is False:
        with open(filename, "w+") as f:
            f.write(content)
    else:
        filename_new = f"{filename}.new"
        print(
            f"Autogenerated file {filename} needs updating. Have you called scripts/autogen?",
            file=sys.stderr,
        )
        print(f"Writing new version to {filename_new}", file=sys.stderr)
        with open(filename_new, "w") as f:
            f.write(content)
        # If the file exists, print diff between old and new version for debugging
        if current_content != None:
            subprocess.run(["diff", filename, filename_new])
        exit(1)


def bitreverse(i, n):
    r = 0
    for _ in range(n):
        r = 2 * r + (i & 1)
        i >>= 1
    return r


def signed_reduce(a):
    """Return signed canonical representative of a mod b"""
    c = a % modulus
    if c >= modulus / 2:
        c -= modulus
    return c


def gen_c_zetas():
    """Generate source and header file for zeta values used in
    the reference NTT and invNTT"""

    # The zeta values are the powers of the chosen root of unity (17),
    # converted to Montgomery form.

    zeta = []
    for i in range(128):
        zeta.append(signed_reduce(pow(root_of_unity, i, modulus) * montgomery_factor))

    # The source code stores the zeta table in bit reversed form
    yield from (zeta[bitreverse(i, 7)] for i in range(128))


def gen_c_zeta_file(dry_run=False):
    def gen():
        yield from gen_header()
        yield "#include <stdint.h>"
        yield ""
        yield "/*"
        yield " * Table of zeta values used in the reference NTT and inverse NTT."
        yield " * See autogen for details."
        yield " */"
        yield "static MLK_ALIGN const int16_t zetas[128] = {"
        yield from map(lambda t: str(t) + ",", gen_c_zetas())
        yield "};"
        yield ""

    update_file("mlkem/zetas.inc", "\n".join(gen()), dry_run=dry_run, force_format=True)


def prepare_root_for_barrett(root):
    """Takes a constant that the code needs to Barrett-multiply with,
    and returns the pair of (a) its signed canonical form, (b) the
    twisted constant used in the high-mul part of the Barrett multiplication."""

    # Signed canonical reduction
    root = signed_reduce(root)

    def round_to_even(t):
        rt = round(t)
        if rt % 2 == 0:
            return rt
        # Make sure to pick a rounding target
        # that's <= 1 away from x in absolute value.
        if rt <= t:
            return rt + 1
        return rt - 1

    root_twisted = round_to_even((root * 2**16) / modulus) // 2
    return root, root_twisted


def gen_aarch64_root_of_unity_for_block(layer, block, inv=False):
    # We are computing a negacyclic NTT; the twiddles needed here is
    # the second half of the twiddles for a cyclic NTT of twice the size.
    # For ease of calculating the roots, layers are numbers 0 through 6
    # in this function.
    log = bitreverse(pow(2, layer) + block, 7)
    if inv is True:
        log = -log
    root, root_twisted = prepare_root_for_barrett(pow(root_of_unity, log, modulus))
    return root, root_twisted


def gen_aarch64_fwd_ntt_zetas_layer12345():
    # Layers 1,2,3 are merged
    yield from gen_aarch64_root_of_unity_for_block(0, 0)
    yield from gen_aarch64_root_of_unity_for_block(1, 0)
    yield from gen_aarch64_root_of_unity_for_block(1, 1)
    yield from gen_aarch64_root_of_unity_for_block(2, 0)
    yield from gen_aarch64_root_of_unity_for_block(2, 1)
    yield from gen_aarch64_root_of_unity_for_block(2, 2)
    yield from gen_aarch64_root_of_unity_for_block(2, 3)
    yield from (0, 0)  # Padding

    # Layers 4,5,6,7 are merged, but we emit roots for 4,5
    # in separate arrays than those for 6,7
    for block in range(8):  # There are 8 blocks in Layer 4
        yield from gen_aarch64_root_of_unity_for_block(3, block)
        yield from gen_aarch64_root_of_unity_for_block(4, 2 * block + 0)
        yield from gen_aarch64_root_of_unity_for_block(4, 2 * block + 1)
        yield from (0, 0)  # Padding


def gen_aarch64_fwd_ntt_zetas_layer67():
    # Layers 4,5,6,7 are merged, but we emit roots for 4,5
    # in separate arrays than those for 6,7
    for block in range(8):

        def double_ith(t, i):
            yield from (t[i], t[i])

        # Ordering of blocks is adjusted to suit the transposed internal
        # presentation of the data
        for i in range(2):
            yield from double_ith(
                gen_aarch64_root_of_unity_for_block(5, 4 * block + 0), i
            )
            yield from double_ith(
                gen_aarch64_root_of_unity_for_block(5, 4 * block + 1), i
            )
            yield from double_ith(
                gen_aarch64_root_of_unity_for_block(5, 4 * block + 2), i
            )
            yield from double_ith(
                gen_aarch64_root_of_unity_for_block(5, 4 * block + 3), i
            )
        for i in range(2):
            yield from double_ith(
                gen_aarch64_root_of_unity_for_block(6, 8 * block + 0), i
            )
            yield from double_ith(
                gen_aarch64_root_of_unity_for_block(6, 8 * block + 2), i
            )
            yield from double_ith(
                gen_aarch64_root_of_unity_for_block(6, 8 * block + 4), i
            )
            yield from double_ith(
                gen_aarch64_root_of_unity_for_block(6, 8 * block + 6), i
            )
        for i in range(2):
            yield from double_ith(
                gen_aarch64_root_of_unity_for_block(6, 8 * block + 1), i
            )
            yield from double_ith(
                gen_aarch64_root_of_unity_for_block(6, 8 * block + 3), i
            )
            yield from double_ith(
                gen_aarch64_root_of_unity_for_block(6, 8 * block + 5), i
            )
            yield from double_ith(
                gen_aarch64_root_of_unity_for_block(6, 8 * block + 7), i
            )


def gen_aarch64_inv_ntt_zetas_layer12345():
    # Layers 4,5,6,7 are merged, but we emit roots for 4,5
    # in separate arrays than those for 6,7
    for block in range(8):  # There are 8 blocks in Layer 4
        yield from gen_aarch64_root_of_unity_for_block(3, block, inv=True)
        yield from gen_aarch64_root_of_unity_for_block(4, 2 * block + 0, inv=True)
        yield from gen_aarch64_root_of_unity_for_block(4, 2 * block + 1, inv=True)
        yield from (0, 0)  # Padding

    # Layers 1,2,3 are merged
    yield from gen_aarch64_root_of_unity_for_block(0, 0, inv=True)
    yield from gen_aarch64_root_of_unity_for_block(1, 0, inv=True)
    yield from gen_aarch64_root_of_unity_for_block(1, 1, inv=True)
    yield from gen_aarch64_root_of_unity_for_block(2, 0, inv=True)
    yield from gen_aarch64_root_of_unity_for_block(2, 1, inv=True)
    yield from gen_aarch64_root_of_unity_for_block(2, 2, inv=True)
    yield from gen_aarch64_root_of_unity_for_block(2, 3, inv=True)
    yield from (0, 0)  # Padding


def gen_aarch64_inv_ntt_zetas_layer67():
    # Layers 4,5,6,7 are merged, but we emit roots for 4,5
    # in separate arrays than those for 6,7
    for block in range(8):

        def double_ith(t, i):
            yield from (t[i], t[i])

        # Ordering of blocks is adjusted to suit the transposed internal
        # presentation of the data
        for i in range(2):
            yield from double_ith(
                gen_aarch64_root_of_unity_for_block(5, 4 * block + 0, inv=True), i
            )
            yield from double_ith(
                gen_aarch64_root_of_unity_for_block(5, 4 * block + 1, inv=True), i
            )
            yield from double_ith(
                gen_aarch64_root_of_unity_for_block(5, 4 * block + 2, inv=True), i
            )
            yield from double_ith(
                gen_aarch64_root_of_unity_for_block(5, 4 * block + 3, inv=True), i
            )
        for i in range(2):
            yield from double_ith(
                gen_aarch64_root_of_unity_for_block(6, 8 * block + 0, inv=True), i
            )
            yield from double_ith(
                gen_aarch64_root_of_unity_for_block(6, 8 * block + 2, inv=True), i
            )
            yield from double_ith(
                gen_aarch64_root_of_unity_for_block(6, 8 * block + 4, inv=True), i
            )
            yield from double_ith(
                gen_aarch64_root_of_unity_for_block(6, 8 * block + 6, inv=True), i
            )
        for i in range(2):
            yield from double_ith(
                gen_aarch64_root_of_unity_for_block(6, 8 * block + 1, inv=True), i
            )
            yield from double_ith(
                gen_aarch64_root_of_unity_for_block(6, 8 * block + 3, inv=True), i
            )
            yield from double_ith(
                gen_aarch64_root_of_unity_for_block(6, 8 * block + 5, inv=True), i
            )
            yield from double_ith(
                gen_aarch64_root_of_unity_for_block(6, 8 * block + 7, inv=True), i
            )


def gen_aarch64_mulcache_twiddles():
    for idx in range(0, 128):
        root = pow(root_of_unity, 2 * bitreverse(idx, 7) + 1, modulus)
        yield prepare_root_for_barrett(root)[0]


def gen_aarch64_mulcache_twiddles_twisted():
    for idx in range(0, 128):
        root = pow(root_of_unity, 2 * bitreverse(idx, 7) + 1, modulus)
        yield prepare_root_for_barrett(root)[1]


def print_hol_light_array(g, as_int=True, entries_per_line=8, pad=0):

    # Format of integer list entries, including `;` separator:
    # - Positive numbers: &42;
    # - Negative numbers: -- &42;
    # If as_int is false, we omit `&` and emit constant as numerals.
    def format_hol_light_int(n):
        prefix = ""
        if n < 0:
            prefix = "-- "
            n = -n
        c = "&" if as_int is True else ""
        return f"{prefix}{c}{n:>{pad}};"

    l = list(map(format_hol_light_int, g))
    # Remove `;` from end of last entry
    l[-1] = l[-1][:-1]

    for i in range(0, len(l), entries_per_line):
        yield "  " + " ".join(l[i : i + entries_per_line])


def gen_aarch64_hol_light_zeta_file(dry_run=False):
    def gen():
        yield from gen_hol_light_header()
        yield "(*"
        yield " * Table of zeta values used in the AArch64 NTTs"
        yield " * See autogen for details."
        yield " *)"
        yield ""
        yield "let ntt_zetas_layer12345 = define `ntt_zetas_layer12345:int list = ["
        yield from print_hol_light_array(gen_aarch64_fwd_ntt_zetas_layer12345())
        yield "]`;;"
        yield ""
        yield "let ntt_zetas_layer67 = define `ntt_zetas_layer67:int list = ["
        yield from print_hol_light_array(gen_aarch64_fwd_ntt_zetas_layer67())
        yield "]`;;"
        yield ""
        yield "let intt_zetas_layer12345 = define `intt_zetas_layer12345:int list = ["
        yield from print_hol_light_array(gen_aarch64_inv_ntt_zetas_layer12345())
        yield "]`;;"
        yield ""
        yield "let intt_zetas_layer67 = define `intt_zetas_layer67:int list = ["
        yield from print_hol_light_array(gen_aarch64_inv_ntt_zetas_layer67())
        yield "]`;;"
        yield ""
        yield "let mulcache_zetas = define `mulcache_zetas:int list = ["
        yield from print_hol_light_array(gen_aarch64_mulcache_twiddles())
        yield "]`;;"
        yield ""
        yield ""
        yield "let mulcache_zetas_twisted = define `mulcache_zetas_twisted:int list = ["
        yield from print_hol_light_array(gen_aarch64_mulcache_twiddles_twisted())
        yield "]`;;"
        yield ""

    update_file(
        "proofs/hol_light/arm/proofs/mlkem_zetas.ml",
        "\n".join(gen()),
        dry_run=dry_run,
    )


def gen_aarch64_zeta_file(dry_run=False):
    def gen():
        yield from gen_header()
        yield '#include "../../../common.h"'
        yield ""
        yield "#if defined(MLK_ARITH_BACKEND_AARCH64) && \\"
        yield "    !defined(MLK_CONFIG_MULTILEVEL_NO_SHARED)"
        yield ""
        yield "#include <stdint.h>"
        yield '#include "arith_native_aarch64.h"'
        yield ""
        yield "/*"
        yield " * Table of zeta values used in the AArch64 forward NTT"
        yield " * See autogen for details."
        yield " */"
        yield "MLK_ALIGN const int16_t mlk_aarch64_ntt_zetas_layer12345[] = {"
        yield from map(lambda t: str(t) + ",", gen_aarch64_fwd_ntt_zetas_layer12345())
        yield "};"
        yield ""
        yield "MLK_ALIGN const int16_t mlk_aarch64_ntt_zetas_layer67[] = {"
        yield from map(lambda t: str(t) + ",", gen_aarch64_fwd_ntt_zetas_layer67())
        yield "};"
        yield ""
        yield "MLK_ALIGN const int16_t mlk_aarch64_invntt_zetas_layer12345[] = {"
        yield from map(lambda t: str(t) + ",", gen_aarch64_inv_ntt_zetas_layer12345())
        yield "};"
        yield ""
        yield "MLK_ALIGN const int16_t mlk_aarch64_invntt_zetas_layer67[] = {"
        yield from map(lambda t: str(t) + ",", gen_aarch64_inv_ntt_zetas_layer67())
        yield "};"
        yield ""
        yield "MLK_ALIGN const int16_t mlk_aarch64_zetas_mulcache_native[] = {"
        yield from map(lambda t: str(t) + ",", gen_aarch64_mulcache_twiddles())
        yield "};"
        yield ""
        yield "MLK_ALIGN const int16_t mlk_aarch64_zetas_mulcache_twisted_native[] = {"
        yield from map(lambda t: str(t) + ",", gen_aarch64_mulcache_twiddles_twisted())
        yield "};"
        yield ""
        yield "#else"
        yield ""
        yield "MLK_EMPTY_CU(aarch64_zetas)"
        yield ""
        yield "#endif"
        yield ""

    update_file(
        "dev/aarch64_opt/src/aarch64_zetas.c",
        "\n".join(gen()),
        dry_run=dry_run,
    )

    update_file(
        "dev/aarch64_clean/src/aarch64_zetas.c",
        "\n".join(gen()),
        dry_run=dry_run,
    )


def gen_aarch64_rej_uniform_table_rows():
    # The index into the lookup table is an 8-bit bitmap, i.e. a number 0..255.
    # Conceptually, the table entry at index i is a vector of 8 16-bit values, of
    # which only the first popcount(i) are set; those are the indices of the set-bits
    # in i. Concretely, we store each 16-bit index as consecutive 8-bit indices.
    def get_set_bits_idxs(i):
        bits = list(map(int, format(i, "08b")))
        bits.reverse()
        return [bit_idx for bit_idx in range(8) if bits[bit_idx] == 1]

    for i in range(256):
        idxs = get_set_bits_idxs(i)
        # Replace each index by two consecutive indices
        idxs = [j for i in idxs for j in [2 * i, 2 * i + 1]]
        # Pad by -1
        idxs = idxs + [255] * (16 - len(idxs))
        yield idxs


def gen_aarch64_hol_light_rej_uniform_table(dry_run=False):
    def gen():
        yield from gen_hol_light_header()
        yield "(*"
        yield " * Constant table values used in the AArch64 rejection sampling."
        yield " * See autogen for details."
        yield " *)"
        yield ""
        yield "let mlkem_rej_uniform_table = (REWRITE_RULE[MAP] o define)"
        yield "  `mlkem_rej_uniform_table:byte list = MAP word ["
        data = [i for idxs in gen_aarch64_rej_uniform_table_rows() for i in idxs]
        yield from print_hol_light_array(data, as_int=False, entries_per_line=16, pad=3)
        yield "]`;;"
        yield ""

    update_file(
        "proofs/hol_light/arm/proofs/mlkem_rej_uniform_table.ml",
        "\n".join(gen()),
        dry_run=dry_run,
    )


def gen_aarch64_rej_uniform_table(dry_run=False):
    def gen():
        yield from gen_header()
        yield '#include "../../../common.h"'
        yield ""
        yield "#if defined(MLK_ARITH_BACKEND_AARCH64) && \\"
        yield "    !defined(MLK_CONFIG_MULTILEVEL_NO_SHARED)"
        yield ""
        yield "#include <stdint.h>"
        yield '#include "arith_native_aarch64.h"'
        yield ""
        yield "/*"
        yield " * Lookup table used by rejection sampling of the public matrix."
        yield " * See autogen for details."
        yield " */"
        yield "MLK_ALIGN const uint8_t mlk_rej_uniform_table[] = {"
        for i, idxs in enumerate(gen_aarch64_rej_uniform_table_rows()):
            yield ",".join(map(str, idxs)) + f" /* {i} */,"
        yield "};"
        yield ""
        yield "#else"
        yield ""
        yield "MLK_EMPTY_CU(aarch64_rej_uniform_table)"
        yield ""
        yield "#endif"
        yield ""

    update_file(
        "dev/aarch64_opt/src/rej_uniform_table.c",
        "\n".join(gen()),
        dry_run=dry_run,
    )

    update_file(
        "dev/aarch64_clean/src/rej_uniform_table.c",
        "\n".join(gen()),
        dry_run=dry_run,
    )


def gen_avx2_rej_uniform_table_rows():
    # The index into the lookup table is an 8-bit bitmap, i.e. a number 0..255.
    # Conceptually, the table entry at index i is a vector of 8 16-bit values, of
    # which only the first popcount(i) are set; those are the indices of the set-bits
    # in i.
    def get_set_bits_idxs(i):
        bits = list(map(int, format(i, "08b")))
        bits.reverse()
        return [bit_idx for bit_idx in range(8) if bits[bit_idx] == 1]

    for i in range(256):
        idxs = get_set_bits_idxs(i)
        idxs = [2 * i for i in idxs]
        # Pad by -1
        idxs = idxs + [-1] * (8 - len(idxs))
        yield "{" + ",".join(map(str, idxs)) + "}"


def gen_avx2_rej_uniform_table(dry_run=False):
    def gen():
        yield from gen_header()
        yield '#include "../../../common.h"'
        yield ""
        yield "#if defined(MLK_ARITH_BACKEND_X86_64_DEFAULT) && \\"
        yield "    !defined(MLK_CONFIG_MULTILEVEL_NO_SHARED)"
        yield ""
        yield "#include <stdint.h>"
        yield '#include "arith_native_x86_64.h"'
        yield ""
        yield "/*"
        yield " * Lookup table used by rejection sampling of the public matrix."
        yield " * See autogen for details."
        yield " */"
        yield "MLK_ALIGN const uint8_t mlk_rej_uniform_table[256][8] = {"
        yield from map(lambda t: str(t) + ",", gen_avx2_rej_uniform_table_rows())
        yield "};"
        yield ""
        yield "#else"
        yield ""
        yield "MLK_EMPTY_CU(avx2_rej_uniform_table)"
        yield ""
        yield "#endif"
        yield ""

    update_file(
        "mlkem/native/x86_64/src/rej_uniform_table.c",
        "\n".join(gen()),
        dry_run=dry_run,
    )


def signed_reduce_u16(x):
    x = x % 2**16
    if x >= 2**15:
        x -= 2**16
    return x


def prepare_root_for_montmul(root):
    """Takes a constant that the code needs to Montgomery-multiply with,
    and returns the pair of (a) the signed canonical representative of its
    Montgomery form, (b) the twisted constant used in the low-mul part of
    the Montgomery multiplication."""

    # Convert to Montgomery form and pick canonical signed representative
    root = signed_reduce(root * montgomery_factor)
    root_twisted = signed_reduce_u16(root * pow(modulus, -1, 2**16))
    return root, root_twisted


def gen_avx2_root_of_unity_for_block(layer, block, inv=False):
    # We are computing a negacyclic NTT; the twiddles needed here is
    # the second half of the twiddles for a cyclic NTT of twice the size.
    log = bitreverse(pow(2, layer) + block, 7)
    if inv is True:
        log = -log
    root, root_twisted = prepare_root_for_montmul(pow(root_of_unity, log, modulus))
    return root, root_twisted


def gen_avx2_fwd_ntt_zetas():

    def gen_twiddles(layer, block, repeat):
        """Generates twisted twiddle, then twiddle, for given layer and block.
        Repeat both the given number of times."""
        root, root_twisted = gen_avx2_root_of_unity_for_block(layer, block)
        return [root] * repeat, [root_twisted] * repeat

    def gen_twiddles_many(layer, block_base, block_offsets, repeat):
        """Generates twisted twiddles, then twiddles, of each (layer, block_base + i)
        pair for i in block_offsets. Each twiddle is repeated `repeat` times."""
        root_pairs = list(
            map(lambda x: gen_twiddles(layer, block_base + x, repeat), block_offsets)
        )
        yield from (r for l in root_pairs for r in l[1])
        yield from (r for l in root_pairs for r in l[0])

    # Layers 1 twiddle
    yield from gen_twiddles_many(0, 0, range(1), 4)
    # Padding so that the subsequent twiddles are 16-byte aligned
    yield from [0] * 8

    # Layer 2-7 twiddles, separated by whether they belong to the upper or lower half
    for i in range(2):
        yield from gen_twiddles_many(1, i * (2**0), range(1), 16)
        yield from gen_twiddles_many(2, i * (2**1), range(2), 8)
        yield from gen_twiddles_many(3, i * (2**2), range(4), 4)
        yield from gen_twiddles_many(4, i * (2**3), range(8), 2)
        yield from gen_twiddles_many(5, i * (2**4), range(16), 1)
        yield from gen_twiddles_many(6, i * (2**5), range(0, 32, 2), 1)
        yield from gen_twiddles_many(6, i * (2**5), range(1, 32, 2), 1)


def gen_avx2_mulcache_twiddles():
    for i in range(2):
        for idx in range(16):
            root, root_twisted = prepare_root_for_montmul(
                pow(root_of_unity, bitreverse(64 + 32 * i + 2 * idx, 7), modulus)
            )
            yield root

        for idx in range(16):
            root, root_twisted = prepare_root_for_montmul(
                pow(root_of_unity, bitreverse(64 + 32 * i + 2 * idx + 1, 7), modulus)
            )
            yield root

    for i in range(2):
        for idx in range(16):
            root, root_twisted = prepare_root_for_montmul(
                pow(root_of_unity, bitreverse(64 + 32 * i + 2 * idx, 7), modulus)
            )
            yield root_twisted

        for idx in range(16):
            root, root_twisted = prepare_root_for_montmul(
                pow(root_of_unity, bitreverse(64 + 32 * i + 2 * idx + 1, 7), modulus)
            )
            yield root_twisted


def gen_avx2_zeta_file(dry_run=False):
    def gen():
        yield from gen_header()
        yield "/*"
        yield " * Table of zeta values used in the AVX2 NTTs"
        yield " * See autogen for details."
        yield " */"
        yield ""
        yield from map(lambda t: str(t) + ",", gen_avx2_fwd_ntt_zetas())
        yield ""

    update_file(
        "mlkem/native/x86_64/src/x86_64_zetas.i", "\n".join(gen()), dry_run=dry_run
    )


def gen_avx2_mulcache_twiddles_file(dry_run=False):
    def gen():
        yield from gen_header()
        yield "/*"
        yield " * Table of twiddle values used in the AVX2 mulcache"
        yield " * See autogen for details."
        yield " */"
        yield ""
        yield from map(lambda t: str(t) + ",", gen_avx2_mulcache_twiddles())
        yield ""

    update_file(
        "dev/x86_64/src/x86_64_mulcache_twiddles.i", "\n".join(gen()), dry_run=dry_run
    )


def get_c_source_files(main_only=False, core_only=False):
    if main_only is True:
        return get_files("mlkem/**/*.c")
    elif core_only is True:
        return get_files("mlkem/**/*.c") + get_files("dev/**/*.c")
    else:
        return get_files("**/*.c")


def get_asm_source_files(main_only=False, core_only=False):
    if main_only is True:
        return get_files("mlkem/**/*.S")
    elif core_only is True:
        return get_files("mlkem/**/*.S") + get_files("dev/**/*.S")
    else:
        return get_files("**/*.S")


def get_header_files(main_only=False, core_only=False):
    if main_only is True:
        return get_files("mlkem/**/*.h")
    elif core_only is True:
        return (
            get_files("mlkem/**/*.h")
            + get_files("dev/**/*.h")
            + get_files("integration/**/*.h")
        )
    else:
        return get_files("**/*.h")


def get_markdown_files(main_only=False):
    return get_files("**/*.md")


def get_files(pattern):
    return [
        str(p)
        for p in pathlib.Path().glob(pattern)
        if p.is_file() and not p.is_symlink()
    ]


def get_defines_from_file(c):
    with open(c, "r") as f:
        for l in f.read().split("\n"):
            if l.lstrip().startswith("#define "):
                yield (
                    c,
                    l.lstrip()
                    .removeprefix("#define ")
                    .split(" ")[0]
                    .split("(")[0]
                    .replace("'", ""),
                )


def get_defines():
    for c in get_header_files(main_only=True):
        yield from get_defines_from_file(c)


def get_checked_defines():
    allow_list = [("__contract__", "cbmc.h"), ("__loop__", "cbmc.h")]

    def is_allowed(d, c):
        for d0, c0 in allow_list:
            if c.endswith(c0) is True and d0 == d:
                return True
        return False

    for c, d in get_defines():
        if d.startswith("_") and is_allowed(d, c) is False:
            raise Exception(
                f"{d} from {c}: starts with an underscore, which is not allowed for mlkem-native macros. "
                f"If this is an mlkem-native specific macro, please pick a different name. "
                f"If this is an external macro, it likely needs removing from `gen_monolithic_undef_all_core()` in `scripts/autogen` -- check this!"
            )
        yield (c, d)


def gen_monolithic_undef_all_core(filt=None, desc=""):

    if filt is None:
        filt = lambda c: True

    yield "/*"
    yield f" * Undefine macros from {desc}"
    yield " */"

    defines = list(set(get_checked_defines()))
    defines.sort()

    last_filename = None
    for filename, d in defines:
        if filt(filename) is False:
            continue
        if last_filename != filename:
            yield f"/* {filename} */"
            last_filename = filename
        yield f"#undef {d}"


def gen_monolithic_source_file(dry_run=False):

    def native(c):
        return "native/" in c

    def fips202(c):
        return "fips202" in c

    def aarch64(c):
        return "aarch64" in c

    def x86_64(c):
        return "x86_64" in c

    def riscv64(c):
        return "riscv64" in c

    def native_fips202(c):
        return native(c) and fips202(c)

    def native_arith(c):
        return native(c) and not fips202(c)

    def native_fips202_aarch64(c):
        return native_fips202(c) and aarch64(c)

    def native_fips202_x86_64(c):
        return native_fips202(c) and x86_64(c)

    def native_arith_aarch64(c):
        return native_arith(c) and aarch64(c)

    def native_arith_riscv64(c):
        return native_arith(c) and riscv64(c)

    def native_arith_x86_64(c):
        return native_arith(c) and x86_64(c)

    # List of level-specific source files
    # All other files only need including and building once
    # in multi-level build.
    def k_specific(c):
        k_specific_sources = [
            # sys.h is not k-specific, but has some macro-overlap with
            # mlkem_native.h. Since the macros from mlkem_native.h are
            # undef'ed after each level-include in a multi-level build
            # we thus have to re-include sys.h as well.
            "sys.h",
            "mlkem_native.h",
            "params.h",
            # Deliberately omit config.h, which is not #undef'ed
            "common.h",
            "indcpa.c",
            "indcpa.h",
            "kem.c",
            "kem.h",
            "poly_k.c",
            "poly_k.h",
        ]
        for f in k_specific_sources:
            if c.endswith(f):
                return True
        return False

    def k_generic(c):
        return not k_specific(c) and c != "mlkem/config.h"

    def gen():
        c_sources = get_c_source_files(main_only=True)
        yield from gen_header()
        yield "/*"
        yield " * Monolithic compilation unit bundling all compilation units within mlkem-native"
        yield " */"
        yield ""
        yield "/******************************************************************************"
        yield " *"
        yield " * Single compilation unit (SCU) for fixed-level build of mlkem-native"
        yield " *"
        yield " * This compilation unit bundles together all source files for a build"
        yield " * of mlkem-native for a fixed security level (MLKEM-512/768/1024)."
        yield " *"
        yield " * # API"
        yield " *"
        yield " * The API exposed by this file is described in mlkem_native.h."
        yield " *"
        yield " * # Multi-level build"
        yield " *"
        yield " * If you want an SCU build of mlkem-native with support for multiple security"
        yield " * levels, you need to include this file multiple times, and set"
        yield " * MLK_CONFIG_MULTILEVEL_WITH_SHARED and MLK_CONFIG_MULTILEVEL_NO_SHARED"
        yield " * appropriately. This is exemplified in examples/monolithic_build_multilevel."
        yield " *"
        yield " * # Configuration"
        yield " *"
        yield " * - MLK_CONFIG_MONOBUILD_CUSTOM_FIPS202"
        yield " *   Set this option if you use a custom FIPS202 implementation."
        yield " *"
        yield " * - MLK_CONFIG_MONOBUILD_WITH_NATIVE_ARITH"
        yield " *   Set this option if you want to include the native arithmetic backends"
        yield " *   in your build."
        yield " *"
        yield " * - MLK_CONFIG_MONOBUILD_WITH_NATIVE_FIPS202"
        yield " *   Set this option if you want to include the native FIPS202 backends"
        yield " *   in your build."
        yield " *"
        yield " * - MLK_CONFIG_MONOBUILD_KEEP_SHARED_HEADERS"
        yield " *   Set this option if you want to keep the directives defined in"
        yield " *   level-independent headers. This is needed for a multi-level build."
        yield " */"
        yield ""
        yield "/* If parts of the mlkem-native source tree are not used,"
        yield " * consider reducing this header via `unifdef`."
        yield " *"
        yield " * Example:"
        yield " * ```bash"
        yield " * unifdef -UMLK_CONFIG_MONOBUILD_WITH_NATIVE_ARITH mlkem_native_monobuild.c"
        yield " * ```"
        yield " */"
        yield ""
        yield '#include "mlkem/sys.h"'
        yield ""
        for c in filter(lambda c: not native(c) and not fips202(c), c_sources):
            yield f'#include "{c}"'
        yield ""
        yield "#if !defined(MLK_CONFIG_MONOBUILD_CUSTOM_FIPS202)"
        for c in filter(lambda c: not native(c) and fips202(c), c_sources):
            yield f'#include "{c}"'
        yield "#endif"
        yield ""
        yield "#if defined(MLK_CONFIG_MONOBUILD_WITH_NATIVE_ARITH)"
        yield "#if defined(MLK_SYS_AARCH64)"
        for c in filter(native_arith_aarch64, c_sources):
            yield f'#include "{c}"'
        yield "#endif"
        yield "#if defined(MLK_SYS_X86_64)"
        for c in filter(native_arith_x86_64, c_sources):
            yield f'#include "{c}"'
        yield "#endif"
        yield "#if defined(MLK_SYS_RISCV64)"
        for c in filter(native_arith_riscv64, c_sources):
            yield f'#include "{c}"'
        yield "#endif"
        yield "#endif"
        yield ""
        yield "#if defined(MLK_CONFIG_MONOBUILD_WITH_NATIVE_FIPS202)"
        yield "#if defined(MLK_SYS_AARCH64)"
        for c in filter(native_fips202_aarch64, c_sources):
            yield f'#include "{c}"'
        yield "#endif"
        yield "#if defined(MLK_SYS_X86_64)"
        for c in filter(native_fips202_x86_64, c_sources):
            yield f'#include "{c}"'
        yield "#endif"
        yield "#endif"
        yield ""
        yield from gen_monolithic_undef_all_core(
            filt=k_specific, desc="MLK_CONFIG_PARAMETER_SET-specific files"
        )
        yield ""
        yield "#if !defined(MLK_CONFIG_MONOBUILD_KEEP_SHARED_HEADERS)"
        yield from gen_monolithic_undef_all_core(
            filt=lambda c: not native(c)
            and k_generic(c)
            and not fips202(c)
            and "cbmc.h" not in c,
            desc="MLK_CONFIG_PARAMETER_SET-generic files",
        )
        # Handle cbmc.h manually -- most #define's therein are only defined when CBMC is set
        # and need not be #undef'ed. In fact, #undef'ing them is risky since their names may
        # well already be occupied.
        yield "/* mlkem/cbmc.h */"
        yield "#undef MLK_CBMC_H"
        yield "#undef __contract__"
        yield "#undef __loop__"
        yield ""
        yield "#if !defined(MLK_CONFIG_MONOBUILD_CUSTOM_FIPS202)"
        yield from gen_monolithic_undef_all_core(
            filt=lambda c: not native(c) and k_generic(c) and fips202(c),
            desc="FIPS-202 files",
        )
        yield "#endif"
        yield ""
        yield "#if defined(MLK_CONFIG_MONOBUILD_WITH_NATIVE_FIPS202)"
        yield from gen_monolithic_undef_all_core(
            filt=native_fips202, desc="native code"
        )
        yield "#endif"
        yield "#if defined(MLK_CONFIG_MONOBUILD_WITH_NATIVE_ARITH)"
        yield from gen_monolithic_undef_all_core(filt=native_arith, desc="native code")
        yield "#endif"
        yield "#endif"
        yield ""

    update_file(
        "examples/monolithic_build/mlkem_native_monobuild.c",
        "\n".join(gen()),
        dry_run=dry_run,
    )


def check_asm_register_aliases_for_file(filename):
    """Checks that `filename` has no mismatching or dangling register aliases"""
    status_update("reg alises", filename)

    def get_alias_def(l):
        s = list(filter(lambda s: s != "", l.strip().split(" ")))
        if len(s) < 3 or s[1] != ".req":
            return None
        return s[0]

    def get_alias_undef(l):
        if l.strip().startswith(".unreq") is False:
            return None
        return list(filter(lambda s: s != "", l.strip().split(" ")))[1]

    with open(filename, "r") as f:
        content = f.read()
    aliases = {}
    for i, l in enumerate(content.split("\n")):
        alias_def = get_alias_def(l)
        alias_undef = get_alias_undef(l)
        if alias_def is not None:
            if alias_def in aliases.keys():
                raise Exception(
                    f"Invalid assembly file {filename}: Duplicate .req directive for {alias_def} at line {i}"
                )
            aliases[alias_def] = i
        elif alias_undef is not None:
            if alias_undef not in aliases.keys():
                raise Exception(
                    f"Invalid assembly file {filename}: .unreq without prior .req for {alias_undef} at line {i}"
                )
            del aliases[alias_undef]

    if len(aliases) > 0:
        fixup_suggestion = [
            "/****************** REGISTER DEALLOCATIONS *******************/"
        ]
        dangling = list(aliases.items())
        # Sort by line number of .req
        dangling.sort(key=lambda s: s[1])

        for a, _ in dangling:
            fixup_suggestion.append(f"    .unreq {a}")
        fixup_suggestion.append("")
        fixup_suggestion = "\n".join(fixup_suggestion)

        raise Exception(
            f"Invalid assembly file {filename}: Dangling .req directives {aliases}.\n\nTry adding this?\n\n{fixup_suggestion}"
        )


def check_asm_register_aliases():
    for asm_file in get_asm_source_files():
        check_asm_register_aliases_for_file(asm_file)


def update_via_simpasm(
    infile_full,
    outdir,
    outfile=None,
    cflags=None,
    preserve_header=True,
    dry_run=False,
    force_cross=False,
):
    status_update("simpasm", infile_full)

    _, infile = os.path.split(infile_full)
    if outfile is None:
        outfile = infile
    outfile_full = os.path.join(outdir, outfile)

    # Check if we need to use a cross-compiler
    if "aarch64" in infile_full:
        source_arch = "aarch64"
    elif "x86_64" in infile_full:
        source_arch = "x86_64"
    else:
        raise Exception(f"Could not detect architecture of source file {infile_full}.")
    # Check native architecture
    if platform.machine().lower() in ["arm64", "aarch64"]:
        native_arch = "aarch64"
    else:
        native_arch = "x86_64"

    if native_arch != source_arch:
        cross_prefix = f"{source_arch}-unknown-linux-gnu-"
        cross_gcc = cross_prefix + "gcc"
        # Check if cross-compiler is present
        if shutil.which(cross_gcc) is None:
            if force_cross is False:
                return
            raise Exception(f"Could not find cross toolchain {cross_prefix}")
    else:
        cross_prefix = None

    with tempfile.NamedTemporaryFile(suffix=".S") as tmp:
        try:
            cmd = [
                "./scripts/simpasm",
                "--objdump=llvm-objdump",
                "-i",
                infile_full,
                "-o",
                tmp.name,
            ]
            if cross_prefix is not None:
                # Stick with llvm-objdump for disassembly
                cmd += ["--cc", cross_prefix + "gcc"]
                cmd += ["--nm", cross_prefix + "nm"]
            if cflags is not None:
                cmd += [f'--cflags="{cflags}"']
            if preserve_header is True:
                cmd += ["-p"]
            r = subprocess.run(
                cmd,
                stdout=subprocess.DEVNULL,
                stderr=subprocess.PIPE,
                check=True,
                text=True,
            )
        except subprocess.CalledProcessError as e:
            print(f"Command failed: {' '.join(cmd)}")
            print(f"Exit code: {e.returncode}")
            print(f"stderr: {e.stderr}")
            raise Exception("Failed to run simpasm") from e
        tmp.seek(0)
        new_contents = tmp.read().decode()

    update_file(outfile_full, new_contents, dry_run=dry_run)


def gen_hol_light_asm_file(job, dry_run=False):
    infile, outfile, indir, cflags = job
    update_via_simpasm(
        f"{indir}/{infile}",
        "proofs/hol_light/arm/mlkem",
        outfile=outfile,
        cflags=cflags,
        preserve_header=False,
        dry_run=dry_run,
    )


def gen_hol_light_asm(dry_run=False):

    joblist = [
        (
            "ntt.S",
            "mlkem_ntt.S",
            "dev/aarch64_opt/src",
            "-march=armv8.4-a+sha3 -Imlkem/native/aarch64/src -DMLK_ARITH_BACKEND_AARCH64_OPT",
        ),
        (
            "intt.S",
            "mlkem_intt.S",
            "dev/aarch64_opt/src",
            "-march=armv8.4-a+sha3 -Imlkem/native/aarch64/src -DMLK_ARITH_BACKEND_AARCH64_OPT",
        ),
        (
            "poly_tomont_asm.S",
            "mlkem_poly_tomont.S",
            "dev/aarch64_opt/src",
            "-march=armv8.4-a+sha3 -Imlkem/native/aarch64/src -DMLK_ARITH_BACKEND_AARCH64_OPT",
        ),
        (
            "poly_tobytes_asm.S",
            "mlkem_poly_tobytes.S",
            "dev/aarch64_opt/src",
            "-march=armv8.4-a+sha3 -Imlkem/native/aarch64/src -DMLK_ARITH_BACKEND_AARCH64_OPT",
        ),
        (
            "poly_reduce_asm.S",
            "mlkem_poly_reduce.S",
            "dev/aarch64_opt/src",
            "-march=armv8.4-a+sha3 -Imlkem/native/aarch64/src -DMLK_ARITH_BACKEND_AARCH64_OPT",
        ),
        (
            "poly_mulcache_compute_asm.S",
            "mlkem_poly_mulcache_compute.S",
            "dev/aarch64_opt/src",
            "-march=armv8.4-a+sha3 -Imlkem/native/aarch64/src -DMLK_ARITH_BACKEND_AARCH64_OPT",
        ),
        (
            "polyvec_basemul_acc_montgomery_cached_asm_k2.S",
            "mlkem_poly_basemul_acc_montgomery_cached_k2.S",
            "dev/aarch64_opt/src",
            "-march=armv8.4-a+sha3 -Imlkem/native/aarch64/src -DMLK_ARITH_BACKEND_AARCH64_OPT",
        ),
        (
            "polyvec_basemul_acc_montgomery_cached_asm_k3.S",
            "mlkem_poly_basemul_acc_montgomery_cached_k3.S",
            "dev/aarch64_opt/src",
            "-march=armv8.4-a+sha3 -Imlkem/native/aarch64/src -DMLK_ARITH_BACKEND_AARCH64_OPT",
        ),
        (
            "polyvec_basemul_acc_montgomery_cached_asm_k4.S",
            "mlkem_poly_basemul_acc_montgomery_cached_k4.S",
            "dev/aarch64_opt/src",
            "-march=armv8.4-a+sha3 -Imlkem/native/aarch64/src -DMLK_ARITH_BACKEND_AARCH64_OPT",
        ),
        (
            "rej_uniform_asm.S",
            "mlkem_rej_uniform.S",
            "dev/aarch64_opt/src",
            "-march=armv8.4-a+sha3 -Imlkem/native/aarch64/src -DMLK_ARITH_BACKEND_AARCH64_OPT",
        ),
        (
            "keccak_f1600_x1_scalar_asm.S",
            "keccak_f1600_x1_scalar.S",
            "dev/fips202/aarch64/src",
            "-march=armv8.4-a+sha3 -Imlkem/fips202/native/aarch64/src/ -DMLK_FIPS202_BACKEND_AARCH64_DEFAULT",
        ),
        (
            "keccak_f1600_x1_v84a_asm.S",
            "keccak_f1600_x1_v84a.S",
            "dev/fips202/aarch64/src",
            "-march=armv8.4-a+sha3 -Imlkem/fips202/native/aarch64/src/ -DMLK_FIPS202_BACKEND_AARCH64_DEFAULT",
        ),
        (
            "keccak_f1600_x2_v84a_asm.S",
            "keccak_f1600_x2_v84a.S",
            "dev/fips202/aarch64/src",
            "-march=armv8.4-a+sha3 -Imlkem/fips202/native/aarch64/src/ -DMLK_FIPS202_BACKEND_AARCH64_DEFAULT",
        ),
        (
            "keccak_f1600_x4_v8a_v84a_scalar_hybrid_asm.S",
            "keccak_f1600_x4_v8a_v84a_scalar.S",
            "dev/fips202/aarch64/src",
            "-march=armv8.4-a+sha3 -Imlkem/fips202/native/aarch64/src/ -DMLK_FIPS202_BACKEND_AARCH64_DEFAULT",
        ),
        (
            "keccak_f1600_x4_v8a_scalar_hybrid_asm.S",
            "keccak_f1600_x4_v8a_scalar.S",
            "dev/fips202/aarch64/src",
            "-march=armv8.4-a+sha3 -Imlkem/fips202/native/aarch64/src/ -DMLK_FIPS202_BACKEND_AARCH64_DEFAULT",
        ),
    ]

    with ThreadPoolExecutor() as executor:
        _ = list(
            executor.map(partial(gen_hol_light_asm_file, dry_run=dry_run), joblist)
        )


def update_via_copy(infile_full, outfile_full, dry_run=False, transform=None):
    status_update("copy", f"{infile_full} -> {outfile_full}")

    with open(infile_full, "r") as f:
        content = f.read()

    if transform is not None:
        content = transform(content)

    update_file(outfile_full, content, dry_run=dry_run)


def update_via_remove(filename, dry_run=False):
    if dry_run is True:
        print(
            f"Autogenerated file {filename} needs removing. Have you called scripts/autogen?",
            file=sys.stderr,
        )
        exit(1)

    # Remove the file
    os.remove(filename)


def synchronize_file(
    f, in_dir, out_dir, dry_run=False, delete=False, no_simplify=False, **kwargs
):

    # Only synchronize sources, but not README.md, Makefile and so on
    extensions = (".c", ".h", ".i", ".inc", ".S")

    if not f.endswith(extensions):
        return None

    basename = os.path.basename(f)

    if delete is True:
        return basename

    if no_simplify is False and f.endswith(".S"):
        update_via_simpasm(f, out_dir, dry_run=dry_run, **kwargs)
    else:
        # Update via copy
        _, infile = os.path.split(f)
        outfile_full = os.path.join(out_dir, infile)
        # The header guards will also be checked later, but if we
        # don't do it here, the dry-run would fail because of a
        # mismatching intermediate file
        if f.endswith(".h"):
            transform = lambda c: adjust_header_guard_for_filename(c, outfile_full)
        else:
            transform = None
        update_via_copy(f, outfile_full, dry_run=dry_run, transform=transform)

    return basename


def synchronize_backend(
    in_dir, out_dir, dry_run=False, delete=False, no_simplify=False, **kwargs
):
    copied = []

    with ThreadPoolExecutor() as executor:
        pool_results = list(
            executor.map(
                partial(
                    synchronize_file,
                    in_dir=in_dir,
                    out_dir=out_dir,
                    dry_run=dry_run,
                    delete=delete,
                    no_simplify=no_simplify,
                    **kwargs,
                ),
                get_files(os.path.join(in_dir, "*")),
            )
        )

    copied = [r for r in pool_results if r is not None]

    if delete is False:
        return

    # Check for files in the target directory that have not been copied
    for f in get_files(os.path.join(out_dir, "*")):
        if os.path.basename(f) in copied:
            continue
        # Otherwise, remove it
        update_via_remove(f, dry_run=dry_run)


def synchronize_backends(
    *, dry_run=False, force_cross=False, clean=False, delete=False, no_simplify=False
):
    if clean is False:
        ty = "opt"
    else:
        ty = "clean"

    if delete is False:
        # We may switch the AArch64 arithmetic backend, so adjust the metadata file
        update_via_copy(
            f"dev/aarch64_{ty}/meta.h",
            "mlkem/native/aarch64/meta.h",
            transform=lambda c: adjust_header_guard_for_filename(
                c, "mlkem/native/aarch64/meta.h"
            ),
        )

    synchronize_backend(
        f"dev/aarch64_{ty}/src",
        "mlkem/native/aarch64/src",
        dry_run=dry_run,
        delete=delete,
        force_cross=force_cross,
        no_simplify=no_simplify,
        cflags="-Imlkem/native/aarch64/src",
    )
    synchronize_backend(
        "dev/fips202/aarch64/src",
        "mlkem/fips202/native/aarch64/src",
        dry_run=dry_run,
        delete=delete,
        force_cross=force_cross,
        no_simplify=no_simplify,
        cflags="-Imlkem/fips202/native/aarch64/src -march=armv8.4-a+sha3",
    )
    synchronize_backend(
        "dev/fips202/aarch64",
        "mlkem/fips202/native/aarch64",
        dry_run=dry_run,
        delete=delete,
        force_cross=force_cross,
        no_simplify=no_simplify,
        cflags="-Imlkem/fips202/native/aarch64 -march=armv8.4-a+sha3",
    )
    synchronize_backend(
        "dev/x86_64/src",
        "mlkem/native/x86_64/src",
        dry_run=dry_run,
        delete=delete,
        force_cross=force_cross,
        no_simplify=no_simplify,
        # Turn off control-flow protection (CET) explicitly. Newer versions of
        # clang turn it on by default and insert endbr64 instructions at every
        # global symbol.
        # We insert endbr64 instruction manually via the MLK_ASM_FN_SYMBOL
        # macro.
        # This leads to duplicate endbr64 instructions causing a failure when
        # comparing the object code before and after simplification.
        cflags="-Imlkem/native/x86_64/src/ -mavx2 -fcf-protection=none",
    )


def adjust_header_guard_for_filename(content, header_file):

    status_update("header guards", header_file)

    content = content.split("\n")
    exceptions = {"mlkem/mlkem_native.h": "MLK_H"}

    # Use full filename as the header guard, with '/' and '.' replaced by '_'
    guard_name = (
        header_file.removeprefix("mlkem/").replace("/", "_").replace(".", "_").upper()
    )
    guard_name = "MLK_" + guard_name

    if header_file in exceptions.keys():
        guard_name = exceptions[header_file]

    def gen_guard():
        yield f"#ifndef {guard_name}"
        yield f"#define {guard_name}"

    def gen_footer():
        yield f"#endif"
        yield ""

    guard = list(gen_guard())
    footer = list(gen_footer())

    # Skip over initial commentary
    insert_at = None
    for i, l in enumerate(content):
        if l.strip() == "" or l.startswith(("/*", " *")):
            continue
        insert_at = i
        break

    i = insert_at
    while content[i].strip() == "":
        i += 1
    # Check if header file has some guard -- if so, drop it
    if content[i].strip().startswith("#if !defined") or content[i].strip().startswith(
        "#ifndef"
    ):
        del content[i]
        if content[i].strip().startswith("#define"):
            del content[i]
        has_guard = True
    else:
        has_guard = False
    # Add standardized guard
    content = content[:i] + guard + content[i:]
    # Check if header has some footer
    if (
        has_guard is True
        and content[-1] == ""
        and content[-2].strip().startswith("#endif")
    ):
        del content[-2:]
    # Add standardized footer
    content = content + footer

    return "\n".join(content)


def gen_header_guard(header_file, dry_run=False):
    with open(header_file, "r") as f:
        content = f.read()
    new_content = adjust_header_guard_for_filename(content, header_file)
    update_file(header_file, new_content, dry_run=dry_run)


def gen_header_guards(dry_run=False):
    with ThreadPoolExecutor() as executor:
        _ = list(
            executor.map(
                partial(gen_header_guard, dry_run=dry_run),
                get_header_files(core_only=True),
            )
        )


def gen_source_undefs(source_file, dry_run=False):
    status_update("undefs", source_file)

    # Get list of #define's clauses in this source file (ignore filename)
    undef_list = list(map(lambda c: c[1], get_defines_from_file(source_file)))
    # Get define clauses from header files, as dict
    header_defs = {d: c for (c, d) in get_defines()}

    undefs = []
    ignored = []
    for d in undef_list:
        if d not in header_defs.keys():
            undefs.append(f"#undef {d}")
        else:
            ignored.append((d, header_defs[d]))
    info_line = "/* Some macros are kept because they are also defined in a header. */"
    if len(ignored) != 0:
        undefs.append(
            "/* Some macros are kept because they are also defined in a header. */"
        )
        for d, c in ignored:
            undefs.append(f"/* Keep: {d} ({c.split('/')[-1]}) */")

    # Remove list of #undef's at the end of the source file, and add
    # the up-to-date one.
    with open(source_file, "r") as f:
        content = f.read().split("\n")
    while True:
        l = content[-1].strip()
        if (
            l.startswith("#undef")
            or l == info_line
            or l.startswith("/* Keep:")
            or l == ""
        ):
            del content[-1]
            continue
        break

    footer = [
        "",
        "/* To facilitate single-compilation-unit (SCU) builds, undefine all macros.",
        " * Don't modify by hand -- this is auto-generated by scripts/autogen. */",
    ]
    # Remove existing footer, if present
    if content[-len(footer) :] == footer:
        content = content[: -len(footer)]

    if len(undefs) != 0:
        content = content + footer + undefs + [""]
    else:
        content = content + [""]

    new_content = "\n".join(content)
    update_file(source_file, new_content, dry_run=dry_run)


def gen_undefs(dry_run=False):
    with ThreadPoolExecutor() as executor:
        _ = list(
            executor.map(
                partial(gen_source_undefs, dry_run=dry_run),
                get_c_source_files(core_only=True),
            )
        )


def gen_slothy(funcs, dry_run=False):
    if dry_run is True:
        # Do nothing for a dry run
        return

    if not isinstance(funcs, list):
        return

    targets = list(map(lambda s: s + ".S", funcs))

    for t in targets:
        status_update("SLOTHY", f"Regenerating {t}")

        if t.startswith("keccak"):
            base = "dev/fips202/aarch64/src"
        else:
            base = "dev/aarch64_opt/src"

        # Remove file(s) to be re-generated
        if t.endswith(".S"):
            subprocess.run(["rm", "-f", f"{base}/{t}"])

        p = subprocess.run(["make", t] + ["-C", base])
        if p.returncode != 0:
            print(f"Failed to run SLOTHY on {t}!")
            exit(1)


class BibliographyEntry:
    def __init__(self, raw_dict):
        self._raw = raw_dict
        self._usages = []

    def register_usages(self, lst):
        self._usages += lst

    @property
    def usages(self):
        return self._usages

    @property
    def name(self):
        return self._raw["name"]

    @property
    def short(self):
        if "short" in self._raw.keys():
            return self._raw["short"]
        return self.name

    @property
    def id(self):
        return self._raw["id"]

    @property
    def url(self):
        return self._raw["url"]

    @staticmethod
    def full_name(name):
        if "," not in name:
            return name
        surname, forename = name.split(",")
        return forename.strip() + " " + surname.strip()

    @property
    def authors(self):
        authors = self._raw["author"]
        if not isinstance(authors, list):
            authors = [authors]
        authors = list(map(BibliographyEntry.full_name, authors))
        return authors

    @property
    def authors_text(self):
        authors = self._raw["author"]
        if not isinstance(authors, list):
            authors = [authors]

        def surname(name):
            return name.split(",")[0].strip()

        if len(authors) > 1:
            authors = ", ".join(map(surname, authors))
        else:
            authors = BibliographyEntry.full_name(authors[0])
        return authors


def gen_markdown_citations_for(filename, bibliography, dry_run=False):

    # Skip BIBLIOGRAPHY.md
    if filename == "BIBLIOGRAPHY.md":
        return

    with open(filename, "r") as f:
        content = f.read()
    content = content.split("\n")

    # Lookup all citations in style `[^ID]`
    citations = {}
    for i, l in enumerate(content):
        for m in re.finditer(r"\[\^(?P<id>\w+)\]", l):
            cite_id = m.group("id")
            uses = citations.get(cite_id, [])
            uses.append((filename, i))
            citations[cite_id] = uses

    # Find and remove any existing citation footnotes
    footnote_footer_start = "<!--- bibliography --->"
    try:
        i = content.index(footnote_footer_start)
        content = content[:i]
    except ValueError:
        pass

    # Add footnotes for all citations found
    if len(citations) > 0:
        content.append(footnote_footer_start)
    cite_ids = list(citations.keys())
    cite_ids.sort()
    for cite_id in cite_ids:
        uses = citations[cite_id]
        entry = bibliography.get(cite_id, None)
        if entry is None:
            raise Exception(
                f"Could not find bibliography entry {cite_id} referenced in {filename}. Known entries: {list(bibliography.keys())}"
            )
        content.append(
            f"[^{cite_id}]: {entry.authors_text}: {entry.name}, [{entry.url}]({entry.url})"
        )

        # Remember this usage of the bibliography entry
        entry.register_usages(uses)

    if len(citations) > 0:
        content.append("")

    update_file(filename, "\n".join(content), dry_run=dry_run)


def gen_c_citations_for(filename, bibliography, dry_run=False):

    with open(filename, "r") as f:
        content = f.read()

    references_start = [
        "/* References",
        " * ==========",
    ]
    references_end = [" */"]

    # Find and remove any existing reference section
    ref_pattern = r"/\* (# )?References.*?\*/\n+"
    content = re.sub(ref_pattern, "", content, flags=re.DOTALL)

    content = content.split("\n")

    # Lookup all citations in style `@[ID]`
    citations = {}
    for i, l in enumerate(content):
        for m in re.finditer(r"@\[(?P<id>\w+)", l):
            cite_id = m.group("id")
            uses = citations.get(cite_id, [])
            # Remember usage. +1 because line counting starts at 1
            uses.append((filename, i + 1))
            citations[cite_id] = uses

    # Add references section
    references = []
    references += references_start

    cite_ids = list(citations.keys())
    cite_ids.sort()
    for cite_id in cite_ids:
        uses = citations[cite_id]
        entry = bibliography.get(cite_id, None)
        if entry is None:
            raise Exception(
                f"Could not find bibliography entry {cite_id} referenced in {filename}"
            )
        references.append(f" *")
        references.append(f" * - [{cite_id}]")
        references.append(f" *   {entry.name}")
        references.append(f" *   {entry.authors_text}")
        references.append(f" *   {entry.url}")

    references += references_end

    # Pre-update formatting messes up overly long comment lines which are
    # likely here. Thus, we explicitly format and fix bad indentation manually.
    references = "\n".join(references)
    references = format_content(references)
    references = re.sub(r" \* (\w.*)", r" *   \1", references)
    references = references.split("\n")
    references = [""] + references

    if len(cite_ids) > 0:
        # Add references to file after initial header section
        # Skip over copyright
        insert_at = None
        for i, l in enumerate(content):
            if l.startswith(("/*", " *")):
                continue
            insert_at = i
            break
        content = content[:insert_at] + references + content[insert_at:]

    # Remember uses -- needs to happen after insertion of references
    # since we need to adjust the line count
    for cite_id in cite_ids:
        uses = citations[cite_id]
        entry = bibliography.get(cite_id, None)

        # Adjust line count after insertion of references
        def bump_line_count(x):
            return (x[0], x[1] + len(references))

        uses = list(map(bump_line_count, uses))

        # Remember this usage of the bibliography entry
        entry.register_usages(uses)

    update_file(filename, "\n".join(content), dry_run=dry_run)


def gen_citations_for(filename, bibliography, dry_run=False):
    if filename.endswith(".md"):
        gen_markdown_citations_for(filename, bibliography, dry_run=dry_run)
    elif filename.endswith((".c", ".h", ".S")):
        gen_c_citations_for(filename, bibliography, dry_run=dry_run)
    else:
        raise Exception(f"Unexpected file extension in {filename}")


def gen_bib_file(bibliography, dry_run=False):

    content = [
        "[//]: # (SPDX-License-Identifier: CC-BY-4.0)",
        "[//]: # (This file is auto-generated from BIBLIOGRAPHY.yml)",
        "[//]: # (Do not modify it directly)",
        "",
        "# Bibliography",
        "",
        "This file lists the citations made throughout the mlkem-native ",
        "source code and documentation.",
        "",
    ]

    cite_ids = list(bibliography.keys())
    cite_ids.sort()

    for cite_id in cite_ids:
        entry = bibliography[cite_id]
        content.append(f"### `{cite_id}`")
        content.append("")
        content.append(f"* {entry.name}")
        content.append(f"* Author(s):")
        for author in entry.authors:
            content.append(f"  - {author}")
        content.append(f"* URL: {entry.url}")
        content.append(f"* Referenced from:")
        # Usages are pairs of (filename, line_count)
        # Ignore line_count for now, as it would require `autogen` after
        # a change to source files.
        files = list(set(map(lambda x: x[0], entry.usages)))
        files.sort()
        for filename in files:
            content.append(f"  - [{filename}]({filename})")
        content.append("")

    update_file("BIBLIOGRAPHY.md", "\n".join(content), dry_run=dry_run)


def gen_citations(dry_run=False):
    # Load bibliography
    with open("BIBLIOGRAPHY.yml", "r") as f:
        bibliography_raw = yaml.safe_load(f.read())

    bibliography = {}
    for r in bibliography_raw:
        cite_id = r["id"]
        bibliography[cite_id] = BibliographyEntry(r)

    with ThreadPoolExecutor() as executor:
        _ = list(
            executor.map(
                partial(
                    gen_citations_for,
                    bibliography=bibliography,
                    dry_run=dry_run,
                ),
                get_markdown_files()
                + get_asm_source_files()
                + get_c_source_files()
                + get_header_files(),
            )
        )

    # Check that every bibliography entry has been used as least once
    for e in bibliography.values():
        if len(e.usages) == 0:
            raise Exception(
                f"Bibliography entry {e.id} is unused! "
                "Add a citation or remove from BIBLIOGRAPHY.yml."
            )

    gen_bib_file(bibliography, dry_run=False)


def _main():
    slothy_choices = [
        "ntt",
        "intt",
        "poly_tobytes_asm",
        "poly_tomont_asm",
        "poly_reduce_asm",
        "poly_mulcache_compute_asm",
        "polyvec_basemul_acc_montgomery_cached_asm_k2",
        "polyvec_basemul_acc_montgomery_cached_asm_k3",
        "polyvec_basemul_acc_montgomery_cached_asm_k4",
        "rej_uniform_asm",
        "keccak_f1600_x1_scalar_asm",
        "keccak_f1600_x4_v8a_scalar_hybrid_asm",
        "keccak_f1600_x4_v8a_v84a_scalar_hybrid_asm",
    ]

    parser = argparse.ArgumentParser(
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument("--dry-run", default=False, action="store_true")
    parser.add_argument("--slothy", nargs="*", default=None, choices=slothy_choices)
    parser.add_argument("--aarch64-clean", default=False, action="store_true")
    parser.add_argument("--no-simplify", default=False, action="store_true")
    parser.add_argument("--force-cross", default=False, action="store_true")

    args = parser.parse_args()

    os.chdir(os.path.join(os.path.dirname(__file__), ".."))

    gen_citations(args.dry_run)

    if args.slothy == []:
        args.slothy = slothy_choices
    gen_slothy(args.slothy, args.dry_run)

    check_asm_register_aliases()

    gen_c_zeta_file(args.dry_run)
    gen_aarch64_hol_light_zeta_file(args.dry_run)
    gen_aarch64_hol_light_rej_uniform_table(args.dry_run)
    gen_aarch64_zeta_file(args.dry_run)
    gen_aarch64_rej_uniform_table(args.dry_run)
    gen_avx2_zeta_file(args.dry_run)
    gen_avx2_rej_uniform_table(args.dry_run)
    gen_avx2_mulcache_twiddles_file(args.dry_run)

    if platform.machine().lower() in ["arm64", "aarch64"]:
        gen_hol_light_asm(args.dry_run)

    synchronize_backends(
        dry_run=args.dry_run,
        clean=args.aarch64_clean,
        no_simplify=args.no_simplify,
        force_cross=args.force_cross,
    )
    gen_header_guards(args.dry_run)
    gen_preprocessor_comments(args.dry_run)
    gen_monolithic_source_file(args.dry_run)
    gen_undefs(args.dry_run)

    synchronize_backends(
        dry_run=args.dry_run,
        clean=args.aarch64_clean,
        delete=True,
        force_cross=args.force_cross,
        no_simplify=args.no_simplify,
    )

    print()


if __name__ == "__main__":
    _main()
