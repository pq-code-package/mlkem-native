/*
 * Copyright (c) The mldsa-native project authors
 * Copyright (c) The mlkem-native project authors
 * Copyright (c) 2025 Arm Limited
 * SPDX-License-Identifier: Apache-2.0 OR ISC OR MIT
 */

#include "../../../../common.h"
#if defined(MLK_FIPS202_ARMV81M_NEED_X4) && \
    !defined(MLK_CONFIG_MULTILEVEL_NO_SHARED)

 .thumb
 .syntax unified
.text


.macro interleave_odds x, t
    vshl.u32    \t, \x, #0
    vshl.u8     u, \t, #2
    vsri.u8     \t, u, #1
    vshl.u8     u, \t, #3
    vsri.u8     \t, u, #2
    vshl.u8     u, \t, #4
    vsri.u8     \t, u, #3
    vshl.u16    u, \t, #8
    vsri.u8     \t, u, #4
    vshl.u32    u, \t, #16
    vsri.u16    \t, u, #8

.endm

.macro interleave_evens x, t
    vshl.u32    \t, \x, #0
    vshr.u8     u, \t, #2
    vsli.u8     \t, u, #1
    vshr.u8     u, \t, #3
    vsli.u8     \t, u, #2
    vshr.u8     u, \t, #4
    vsli.u8     \t, u, #3
    vshr.u16    u, \t, #8
    vsli.u8     \t, u, #4
    vshr.u32    u, \t, #16
    vsli.u16    \t, u, #8
.endm

.align 8
.type to_bit_interleaving_4x, %function
.global to_bit_interleaving_4x
to_bit_interleaving_4x:
    push	{r4 - r11, lr}
    vpush   {d8 - d15}
        to_bit_interleaving_4x_start:
                                    // Instructions:    48
                                    // Expected cycles: 95
                                    // Expected IPC:    0.51
                                    //
                                    // Cycle bound:     95.0
                                    // IPC bound:       0.51
                                    //
                                    // Wall time:     0.07s
                                    // User time:     0.07s
                                    //
                                    // -------------------------------------- cycle (expected) -------------------------------------->
                                    // 0                        25                       50                       75
                                    // |------------------------|------------------------|------------------------|-------------------
        vshl.u32 q7, q0, #0         // *..............................................................................................
        vshl.u32 q2, q0, #0         // ..*............................................................................................
        vshl.u32 q6, q1, #0         // ....*..........................................................................................
        vshl.u32 q0, q1, #0         // ......*........................................................................................
        vshl.u8 q3, q7, #2          // ........*......................................................................................
        vsri.u8 q7, q3, #1          // ..........*....................................................................................
        vshr.u8 q1, q2, #2          // ............*..................................................................................
        vsli.u8 q2, q1, #1          // ..............*................................................................................
        vshl.u8 q5, q6, #2          // ................*..............................................................................
        vsri.u8 q6, q5, #1          // ..................*............................................................................
        vshr.u8 q4, q0, #2          // ....................*..........................................................................
        vsli.u8 q0, q4, #1          // ......................*........................................................................
        vshl.u8 q5, q7, #3          // ........................*......................................................................
        vsri.u8 q7, q5, #2          // ..........................*....................................................................
        vshr.u8 q1, q2, #3          // ............................*..................................................................
        vsli.u8 q2, q1, #2          // ..............................*................................................................
        vshl.u8 q3, q6, #3          // ................................*..............................................................
        vsri.u8 q6, q3, #2          // ..................................*............................................................
        vshr.u8 q5, q0, #3          // ....................................*..........................................................
        vsli.u8 q0, q5, #2          // ......................................*........................................................
        vshl.u8 q1, q7, #4          // ........................................*......................................................
        vsri.u8 q7, q1, #3          // ..........................................*....................................................
        vshr.u8 q5, q2, #4          // ............................................*..................................................
        vsli.u8 q2, q5, #3          // ..............................................*................................................
        vshl.u8 q5, q6, #4          // ................................................*..............................................
        vsri.u8 q6, q5, #3          // ..................................................*............................................
        vshr.u8 q5, q0, #4          // ....................................................*..........................................
        vsli.u8 q0, q5, #3          // ......................................................*........................................
        vshl.u16 q4, q7, #8         // ........................................................*......................................
        vsri.u8 q7, q4, #4          // ..........................................................*....................................
        vshr.u16 q3, q2, #8         // ............................................................*..................................
        vsli.u8 q2, q3, #4          // ..............................................................*................................
        vshl.u16 q1, q6, #8         // ................................................................*..............................
        vsri.u8 q6, q1, #4          // ..................................................................*............................
        vshr.u16 q3, q0, #8         // ....................................................................*..........................
        vsli.u8 q0, q3, #4          // ......................................................................*........................
        vshl.u32 q3, q7, #16        // ........................................................................*......................
        vsri.u16 q7, q3, #8         // ..........................................................................*....................
        vshr.u32 q4, q2, #16        // ............................................................................*..................
        vsli.u16 q2, q4, #8         // ..............................................................................*................
        vshl.u32 q3, q6, #16        // ................................................................................*..............
        vsri.u16 q6, q3, #8         // ..................................................................................*............
        vshr.u32 q4, q0, #16        // ....................................................................................*..........
        vsli.u16 q0, q4, #8         // ......................................................................................*........
        vsri.32 q6, q7, #16         // ........................................................................................*......
        vsli.32 q2, q0, #16         // ..........................................................................................*....
        vshl.u32 q1, q6, #0         // ............................................................................................*..
        vshl.u32 q0, q2, #0         // ..............................................................................................*

                                             // -------------------------------------- cycle (expected) -------------------------------------->
                                             // 0                        25                       50                       75
                                             // |------------------------|------------------------|------------------------|-------------------
        // vshl.u32    E0, q0, #0            // ..*............................................................................................
        // vshr.u8     u, E0, #2             // ............*..................................................................................
        // vsli.u8     E0, u, #1             // ..............*................................................................................
        // vshr.u8     u, E0, #3             // ............................*..................................................................
        // vsli.u8     E0, u, #2             // ..............................*................................................................
        // vshr.u8     u, E0, #4             // ............................................*..................................................
        // vsli.u8     E0, u, #3             // ..............................................*................................................
        // vshr.u16    u, E0, #8             // ............................................................*..................................
        // vsli.u8     E0, u, #4             // ..............................................................*................................
        // vshr.u32    u, E0, #16            // ............................................................................*..................
        // vsli.u16    E0, u, #8             // ..............................................................................*................
        // vshl.u32    E1, q1, #0            // ......*........................................................................................
        // vshr.u8     u, E1, #2             // ....................*..........................................................................
        // vsli.u8     E1, u, #1             // ......................*........................................................................
        // vshr.u8     u, E1, #3             // ....................................*..........................................................
        // vsli.u8     E1, u, #2             // ......................................*........................................................
        // vshr.u8     u, E1, #4             // ....................................................*..........................................
        // vsli.u8     E1, u, #3             // ......................................................*........................................
        // vshr.u16    u, E1, #8             // ....................................................................*..........................
        // vsli.u8     E1, u, #4             // ......................................................................*........................
        // vshr.u32    u, E1, #16            // ....................................................................................*..........
        // vsli.u16    E1, u, #8             // ......................................................................................*........
        // vsli.32          E0, E1, #16      // ..........................................................................................*....
        // vshl.u32    O0, q0, #0            // *..............................................................................................
        // vshl.u8     u, O0, #2             // ........*......................................................................................
        // vsri.u8     O0, u, #1             // ..........*....................................................................................
        // vshl.u8     u, O0, #3             // ........................*......................................................................
        // vsri.u8     O0, u, #2             // ..........................*....................................................................
        // vshl.u8     u, O0, #4             // ........................................*......................................................
        // vsri.u8     O0, u, #3             // ..........................................*....................................................
        // vshl.u16    u, O0, #8             // ........................................................*......................................
        // vsri.u8     O0, u, #4             // ..........................................................*....................................
        // vshl.u32    u, O0, #16            // ........................................................................*......................
        // vsri.u16    O0, u, #8             // ..........................................................................*....................
        // vshl.u32    O1, q1, #0            // ....*..........................................................................................
        // vshl.u8     u, O1, #2             // ................*..............................................................................
        // vsri.u8     O1, u, #1             // ..................*............................................................................
        // vshl.u8     u, O1, #3             // ................................*..............................................................
        // vsri.u8     O1, u, #2             // ..................................*............................................................
        // vshl.u8     u, O1, #4             // ................................................*..............................................
        // vsri.u8     O1, u, #3             // ..................................................*............................................
        // vshl.u16    u, O1, #8             // ................................................................*..............................
        // vsri.u8     O1, u, #4             // ..................................................................*............................
        // vshl.u32    u, O1, #16            // ................................................................................*..............
        // vsri.u16    O1, u, #8             // ..................................................................................*............
        // vsri.32          O1, O0, #16      // ........................................................................................*......
        // vshl.u32         q0, E0, #0       // ..............................................................................................*
        // vshl.u32         q1, O1, #0       // ............................................................................................*..

        to_bit_interleaving_4x_exit:

    vpop {d8 - d15}
    pop {r4 - r11, pc}

.macro deinterleave_inner t
    vsli.u32     \t, \t, #8
    vsli.u16     \t, \t, #4
    vshr.u8       u, \t, #3
    vsli.u8      \t,  u, #4
    vshr.u8       u, \t, #2
    vsli.u8      \t,  u, #3
    vshr.u8       u, \t, #1
    vsli.u8      \t,  u, #2
.endm

.align 8
.type from_bit_interleaving_4x, %function
.global from_bit_interleaving_4x
from_bit_interleaving_4x:
    push	{r4 - r11, lr}
    vpush   {d8 - d15}
        from_bit_interleaving_4x_start:
                                    // Instructions:    44
                                    // Expected cycles: 85
                                    // Expected IPC:    0.52
                                    //
                                    // Cycle bound:     85.0
                                    // IPC bound:       0.52
                                    //
                                    // Wall time:     0.04s
                                    // User time:     0.04s
                                    //
                                    // --------------------------------- cycle (expected) --------------------------------->
                                    // 0                        25                       50                       75
                                    // |------------------------|------------------------|------------------------|---------
        vshr.u32 q2, q0, #16        // *....................................................................................
        mov r6, #0x55               // .*...................................................................................
        vsli.u32 q0, q0, #8         // ..*..................................................................................
        vshr.u32 q6, q1, #16        // ....*................................................................................
        vsli.u32 q1, q1, #8         // ......*..............................................................................
        vsli.u32 q2, q2, #8         // ........*............................................................................
        vsli.u16 q0, q0, #4         // ..........*..........................................................................
        vsli.u32 q6, q6, #8         // ............*........................................................................
        vsli.u16 q1, q1, #4         // ..............*......................................................................
        vsli.u16 q2, q2, #4         // ................*....................................................................
        vshr.u8 q5, q0, #3          // ..................*..................................................................
        vsli.u8 q0, q5, #4          // ....................*................................................................
        vsli.u16 q6, q6, #4         // ......................*..............................................................
        vshr.u8 q7, q1, #3          // ........................*............................................................
        vsli.u8 q1, q7, #4          // ..........................*..........................................................
        vshr.u8 q5, q2, #3          // ............................*........................................................
        vsli.u8 q2, q5, #4          // ..............................*......................................................
        vshr.u8 q4, q0, #2          // ................................*....................................................
        vsli.u8 q0, q4, #3          // ..................................*..................................................
        vshr.u8 q3, q6, #3          // ....................................*................................................
        vsli.u8 q6, q3, #4          // ......................................*..............................................
        vshr.u8 q7, q1, #2          // ........................................*............................................
        vsli.u8 q1, q7, #3          // ..........................................*..........................................
        vshr.u8 q4, q2, #2          // ............................................*........................................
        vsli.u8 q2, q4, #3          // ..............................................*......................................
        vshr.u8 q5, q0, #1          // ................................................*....................................
        vsli.u8 q0, q5, #2          // ..................................................*..................................
        vshr.u8 q7, q6, #2          // ....................................................*................................
        vsli.u8 q6, q7, #3          // ......................................................*..............................
        vshr.u8 q5, q1, #1          // ........................................................*............................
        vsli.u8 q1, q5, #2          // ..........................................................*..........................
        vshr.u8 q4, q2, #1          // ............................................................*........................
        vsli.u8 q2, q4, #2          // ..............................................................*......................
        vdup.u8 q4, r6              // ................................................................*....................
        vand.u32 q7, q0, q4         // ..................................................................*..................
        vshr.u8 q3, q6, #1          // ....................................................................*................
        vsli.u8 q6, q3, #2          // ......................................................................*..............
        vand.u32 q1, q1, q4         // ........................................................................*............
        vand.u32 q3, q2, q4         // ..........................................................................*..........
        vand.u32 q6, q6, q4         // ............................................................................*........
        vshl.u32 q2, q1, #1         // ..............................................................................*......
        vorr q0, q7, q2             // ................................................................................*....
        vshl.u32 q2, q6, #1         // ..................................................................................*..
        vorr q1, q3, q2             // ....................................................................................*

                                         // --------------------------------- cycle (expected) --------------------------------->
                                         // 0                        25                       50                       75
                                         // |------------------------|------------------------|------------------------|---------
        // vshr.u32     E1, q0, #16      // *....................................................................................
        // vsli.u32     E1, E1, #8       // ........*............................................................................
        // vsli.u16     E1, E1, #4       // ................*....................................................................
        // vshr.u8       u, E1, #3       // ............................*........................................................
        // vsli.u8      E1,  u, #4       // ..............................*......................................................
        // vshr.u8       u, E1, #2       // ............................................*........................................
        // vsli.u8      E1,  u, #3       // ..............................................*......................................
        // vshr.u8       u, E1, #1       // ............................................................*........................
        // vsli.u8      E1,  u, #2       // ..............................................................*......................
        // vshr.u32     O1, q1, #16      // ....*................................................................................
        // vsli.u32     O1, O1, #8       // ............*........................................................................
        // vsli.u16     O1, O1, #4       // ......................*..............................................................
        // vshr.u8       u, O1, #3       // ....................................*................................................
        // vsli.u8      O1,  u, #4       // ......................................*..............................................
        // vshr.u8       u, O1, #2       // ....................................................*................................
        // vsli.u8      O1,  u, #3       // ......................................................*..............................
        // vshr.u8       u, O1, #1       // ....................................................................*................
        // vsli.u8      O1,  u, #2       // ......................................................................*..............
        // mov          r, #0x55         // .*...................................................................................
        // vdup.u8      M, r             // ................................................................*....................
        // vand.u32     E1, E1, M        // ..........................................................................*..........
        // vand.u32     O1, O1, M        // ............................................................................*........
        // vsli.u32     q0, q0, #8       // ..*..................................................................................
        // vsli.u16     q0, q0, #4       // ..........*..........................................................................
        // vshr.u8       u, q0, #3       // ..................*..................................................................
        // vsli.u8      q0,  u, #4       // ....................*................................................................
        // vshr.u8       u, q0, #2       // ................................*....................................................
        // vsli.u8      q0,  u, #3       // ..................................*..................................................
        // vshr.u8       u, q0, #1       // ................................................*....................................
        // vsli.u8      q0,  u, #2       // ..................................................*..................................
        // vsli.u32     q1, q1, #8       // ......*..............................................................................
        // vsli.u16     q1, q1, #4       // ..............*......................................................................
        // vshr.u8       u, q1, #3       // ........................*............................................................
        // vsli.u8      q1,  u, #4       // ..........................*..........................................................
        // vshr.u8       u, q1, #2       // ........................................*............................................
        // vsli.u8      q1,  u, #3       // ..........................................*..........................................
        // vshr.u8       u, q1, #1       // ........................................................*............................
        // vsli.u8      q1,  u, #2       // ..........................................................*..........................
        // vand.u32     q0, q0, M        // ..................................................................*..................
        // vand.u32     q1, q1, M        // ........................................................................*............
        // vshl.u32     q1, q1, #1       // ..............................................................................*......
        // vorr         q0, q0, q1       // ................................................................................*....
        // vshl.u32     O1, O1, #1       // ..................................................................................*..
        // vorr         q1, E1, O1       // ....................................................................................*

        from_bit_interleaving_4x_exit:

    vpop {d8 - d15}
    pop {r4 - r11, pc}


@ ----------------------------------------------------------------------------
@
@ uint32x4_t KeccakF1600x4_StateXORBytes_aligned(uint32_t nvecs, uint8_t* state, uint32x4_t data_ptrs)
@ WARNING: Assumes that length limits are already applied
@ WARNING: Assumes that state is offset by -16

.align 8
.global   KeccakF1600x4_StateXORBytes_aligned
.type KeccakF1600x4_StateXORBytes_aligned,%function
KeccakF1600x4_StateXORBytes_aligned:
    push {r4, lr}
    vpush {d14 - d15}
    @ Setup the two state pointers
    add r2, r1, #400
    @ Offset the data pointers
    mov r3, #4
    vsub.u32 q7, q0, r3
    @ vmov q7, q0
    @ mov lr, r0
    @ xor each vector into the state
    wls lr, r0, x4_aligned_loop_end
x4_aligned_loop_start:
        @ load each vector
    vldrw.u32 q0, [q7, #4]!
    vldrw.u32 q1, [q7, #4]!
        @ Convert to bit-interleaved representation
    mov r4, lr
    bl to_bit_interleaving_4x
    mov lr, r4
        @ load state
    vldrw.u32 q2, [r1, #16]
    vldrw.u32 q3, [r2, #16]
        @ xor bit interleaved vector with state
    veor      q2, q2, q0
    veor      q3, q3, q1
        @ write state back
    vstrw.u32 q2, [r1, #16]!
    vstrw.u32 q3, [r2, #16]!
        @ decrement length
    le  lr, x4_aligned_loop_start
x4_aligned_loop_end:
    vadd.u32 q0, q7, r3
    vpop {d14 - d15}
    pop {r4, pc}


@ uint32x4x2_t KeccakF1600x4_LoadBytesInLane(uint32x4_t data_ptrs, uint32_t length, uint32_t offset)
.align 8
.global   KeccakF1600x4_LoadBytesInLane
.type KeccakF1600x4_LoadBytesInLane,%function
KeccakF1600x4_LoadBytesInLane:
    push {r4-r11, lr}
    vpush {d8-d11}
    @ generate incremeting sequence
    mov r2, #0
    vidup.u8 q1, r2, #1
    @ calculate the predicates
    @ mask = (1 << length) - 1
    mov r3, #1
    lsl r3, r3, r0
    sub r3, r3, #1
    @ mask << offset
    lsl r3, r3, r1
    vmsr p0, r3
    @ subtract the offset from the addresses to match the predicate
    vsub.u32 q0, q0, r1
    @ get the addresses
    vmov r4, r6, q0[2], q0[0]
    vmov r5, r7, q0[3], q0[1]
    @ now load the partial lanes
    vpstttt
    vldrbt.u8 q2, [r4, q1]
    vldrbt.u8 q3, [r5, q1]
    vldrbt.u8 q4, [r6, q1]
    vldrbt.u8 q5, [r7, q1]
    @ sort the partial lanes into two 4x32 vectors
    vmov r8, r9, d4
    vmov r10, r11, d8
    vmov q0[2], q0[0], r8, r10
    vmov q1[2], q1[0], r9, r11

    vmov r8, r9, d6
    vmov r10, r11, d10
    vmov q0[3], q0[1], r8, r10
    vmov q1[3], q1[1], r9, r11

    vpop {d8 - d11}
    pop {r4-r11, pc}

#endif /* MLK_FIPS202_ARMV81M_NEED_X4 && !MLK_CONFIG_MULTILEVEL_NO_SHARED */
